{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.10.8 | packaged by conda-forge | (main, Nov 22 2022, 08:25:13) [Clang 14.0.6 ]\n",
      "sys.version_info(major=3, minor=10, micro=8, releaselevel='final', serial=0)\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version)\n",
    "print(sys.version_info)\n",
    "print(sys.version_info < (3, 0))\n",
    "\n",
    "\n",
    "num_proc = 8\n",
    "batch_size = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "from datasets import load_dataset # huggingface datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset wikipedia (/Users/seven/.cache/huggingface/datasets/wikipedia/20220301.en/2.0.0/aa542ed919df55cc5d3347f42dd4521d05ca68751f50dbc32bae2a7f1e167559)\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "355fe0676b3b437da05c24a30a90b6dc"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_wikipedia_en = load_dataset(\"wikipedia\", \"20220301.en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached split indices for dataset at /Users/seven/.cache/huggingface/datasets/wikipedia/20220301.en/2.0.0/aa542ed919df55cc5d3347f42dd4521d05ca68751f50dbc32bae2a7f1e167559/cache-6cea40a8991b1426.arrow and /Users/seven/.cache/huggingface/datasets/wikipedia/20220301.en/2.0.0/aa542ed919df55cc5d3347f42dd4521d05ca68751f50dbc32bae2a7f1e167559/cache-4b3c5592cb6498e7.arrow\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset_wikipedia_en\n",
    "split_dataset = dataset[\"train\"].train_test_split(test_size=0.0005, seed=2357, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'url', 'title', 'text'],\n",
      "        num_rows: 6455440\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'url', 'title', 'text'],\n",
      "        num_rows: 3230\n",
      "    })\n",
      "})\n",
      "Dataset({\n",
      "    features: ['id', 'url', 'title', 'text'],\n",
      "    num_rows: 6455440\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(split_dataset)\n",
    "print(split_dataset['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6455440\n"
     ]
    }
   ],
   "source": [
    "print(len(split_dataset['train']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import threading\n",
    "\n",
    "class ThreadSafeCounter:\n",
    "    def __init__(self):\n",
    "        self.count = 0\n",
    "        self.lock = threading.Lock()\n",
    "\n",
    "    def increment(self, value: int):\n",
    "        with self.lock:\n",
    "            self.count += value\n",
    "\n",
    "    def get_count(self):\n",
    "        with self.lock:\n",
    "            return self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6458670\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/6458670 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "89acd7c8f014492a93498457e5065454"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function process_batch at 0x144ed9240> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "text/plain": "tokenizing the dataset (num_proc=8):   0%|          | 0/6458670 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ffb770df0156459c9fce56aa4ab6127e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.auto import trange, tqdm\n",
    "\n",
    "total_len = len(dataset_wikipedia_en['train'])\n",
    "print(total_len)\n",
    "total_progress = tqdm(total=total_len)\n",
    "\n",
    "train = split_dataset['train']\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "counter = ThreadSafeCounter()\n",
    "\n",
    "def process_batch(examples):\n",
    "    ids = enc.encode_batch([example for example in examples['text']])\n",
    "    eos_tokens = [enc.eot_token] * len(ids)\n",
    "    ids = [id + [eos_token] for id, eos_token in zip(ids, eos_tokens)]\n",
    "    lens = [len(id) for id in ids]\n",
    "    out = {'ids': ids, 'len': lens}\n",
    "    counter.increment(batch_size)\n",
    "    total_progress.update(total_len/counter.get_count())\n",
    "    total_progress.refresh()\n",
    "    return out\n",
    "\n",
    "# tokenize the dataset using batch processing\n",
    "tokenized = dataset_wikipedia_en.map(\n",
    "    process_batch,\n",
    "    batched=True,\n",
    "    batch_size=batch_size,\n",
    "    remove_columns=['text'],\n",
    "    num_proc=num_proc,\n",
    "    desc=\"tokenizing the dataset\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "DatasetDict({\n    train: Dataset({\n        features: ['id', 'url', 'title', 'ids', 'len'],\n        num_rows: 6458670\n    })\n})"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "use_slow = False\n",
    "if use_slow:\n",
    "    enc = tiktoken.get_encoding(\"gpt2\")\n",
    "    def process(example):\n",
    "        ids = enc.encode_ordinary(example['text']) # encode_ordinary ignores any special tokens\n",
    "        ids.append(enc.eot_token) # add the end of text token, e.g. 50256 for gpt2 bpe\n",
    "        # note: I think eot should be prepended not appended... hmm. it's called \"eot\" though...\n",
    "        out = {'ids': ids, 'len': len(ids)}\n",
    "\n",
    "        batch_size = 1\n",
    "        counter.increment(batch_size)\n",
    "        total_progress.update(total_len/counter.get_count())\n",
    "        total_progress.refresh()\n",
    "\n",
    "        return out\n",
    "\n",
    "    tokenized = split_dataset.map(\n",
    "        process,\n",
    "        remove_columns=['text'],\n",
    "        desc=\"tokenizing the splits\",\n",
    "        num_proc=num_proc,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "'/Users/seven/Documents/ai/nanoGPT'"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "try:\n",
    "    folder = os.path.dirname(__file__)\n",
    "except:\n",
    "    folder = os.getcwd()\n",
    "folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "dict_items([('train', Dataset({\n    features: ['id', 'url', 'title', 'ids', 'len'],\n    num_rows: 6458670\n}))])"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized.items()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing /Users/seven/Documents/ai/nanoGPT/train.bin...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not broadcast input array from shape (8817,) into shape (1712,)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[17], line 19\u001B[0m\n\u001B[1;32m     17\u001B[0m buffer \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mzeros(buffer_size, dtype\u001B[38;5;241m=\u001B[39mdtype)\n\u001B[1;32m     18\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i, example \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(dset):\n\u001B[0;32m---> 19\u001B[0m     \u001B[43mbuffer\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m \u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43midx\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mexample\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mlen\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m]\u001B[49m \u001B[38;5;241m=\u001B[39m example[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mids\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[1;32m     20\u001B[0m     idx \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m example[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlen\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[1;32m     21\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m idx \u001B[38;5;241m>\u001B[39m\u001B[38;5;241m=\u001B[39m buffer_size:\n",
      "\u001B[0;31mValueError\u001B[0m: could not broadcast input array from shape (8817,) into shape (1712,)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# # concatenate all the ids in the dataset into one large file we can use for training\n",
    "# concatenate all the ids in each dataset into one large file we can use for training\n",
    "for split, dset in tokenized.items():\n",
    "    arr_len = np.sum(dset['len'])\n",
    "    filename = os.path.join(os.path.dirname(__file__), f'{split}.bin')\n",
    "    dtype = np.uint16 # (can do since enc.max_token_value == 50256 is < 2**16)\n",
    "    arr = np.memmap(filename, dtype=dtype, mode='w+', shape=(arr_len,))\n",
    "\n",
    "    print(f\"writing {filename}...\")\n",
    "    idx = 0\n",
    "    for example in tqdm(dset):\n",
    "        arr[idx : idx + example['len']] = example['ids']\n",
    "        idx += example['len']\n",
    "    arr.flush()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('done ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "nanogpt",
   "language": "python",
   "display_name": "nanogpt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "dedde7c3aef9f13b78e4d1bd3da388942376b9fa628f77c25597e4ef98a8d24d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
