{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will not use ddp\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "\n",
    "\"\"\"\n",
    "This training script can be run both on a single gpu in debug mode,\n",
    "and also in a larger training run with distributed data parallel (ddp).\n",
    "\n",
    "To run on a single GPU, example:\n",
    "$ python train.py --batch_size=32 --compile=False\n",
    "\n",
    "To run with DDP on 4 gpus on 1 node, example:\n",
    "$ torchrun --standalone --nproc_per_node=4 train.py\n",
    "\n",
    "To run with DDP on 4 gpus across 2 nodes, example:\n",
    "- Run on the first (master) node with example IP 123.456.123.456:\n",
    "$ torchrun --nproc_per_node=8 --nnodes=2 --node_rank=0 --master_addr=123.456.123.456 --master_port=1234 train.py\n",
    "- Run on the worker node:\n",
    "$ torchrun --nproc_per_node=8 --nnodes=2 --node_rank=1 --master_addr=123.456.123.456 --master_port=1234 train.py\n",
    "(If your cluster does not have Infiniband interconnect prepend NCCL_IB_DISABLE=1)\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "import pickle\n",
    "import inspect\n",
    "from contextlib import nullcontext\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.distributed import init_process_group, destroy_process_group\n",
    "\n",
    "from model import GPTConfig, GPT\n",
    "\n",
    "import minlora\n",
    "# -----------------------------------------------------------------------------\n",
    "# default config values designed to train a gpt2 (124M) on OpenWebText\n",
    "# I/O\n",
    "out_dir = 'out'\n",
    "eval_interval = 200\n",
    "log_interval = 1\n",
    "eval_iters = 20\n",
    "eval_only = False # if True, script exits right after the first eval\n",
    "always_save_checkpoint = True # if True, always save a checkpoint after each eval\n",
    "init_from = 'scratch' # 'scratch' or 'resume' or 'gpt2*'\n",
    "# wandb logging\n",
    "wandb_log = False # disabled by default\n",
    "wandb_project = 'united'\n",
    "wandb_run_name = 'gpt2' # 'run' + str(time.time())\n",
    "# data\n",
    "dataset = 'openwebtext'\n",
    "gradient_accumulation_steps = 5 # used to simulate larger batch sizes\n",
    "batch_size = 3 # if gradient_accumulation_steps > 1, this is the micro-batch size\n",
    "block_size = 1024\n",
    "# model\n",
    "n_layer = 12\n",
    "n_head = 12\n",
    "n_embd = 768\n",
    "dropout = 0.0 # for pretraining 0 is good, for finetuning try 0.1+\n",
    "bias = False # do we use bias inside LayerNorm and Linear layers?\n",
    "# adamw optimizer\n",
    "learning_rate = 6e-4 # max learning rate\n",
    "max_iters = 600000 # total number of training iterations\n",
    "weight_decay = 1e-1\n",
    "beta1 = 0.9\n",
    "beta2 = 0.95\n",
    "grad_clip = 1.0 # clip gradients at this value, or disable if == 0.0\n",
    "# learning rate decay settings\n",
    "decay_lr = True # whether to decay the learning rate\n",
    "warmup_iters = 20 # how many steps to warm up for\n",
    "lr_decay_iters = 600000 # should be ~= max_iters per Chinchilla\n",
    "min_lr = 6e-5 # minimum learning rate, should be ~= learning_rate/10 per Chinchilla\n",
    "# DDP settings\n",
    "backend = 'nccl' # 'nccl', 'gloo', etc.\n",
    "# system\n",
    "device = 'mps' # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1' etc., or try 'mps' on macbooks\n",
    "dtype = 'float32' # 'float32', 'bfloat16', or 'float16', the latter will auto implement a GradScaler\n",
    "compile = False # use PyTorch 2.0 to compile the model to be faster\n",
    "use_lora = True\n",
    "# -----------------------------------------------------------------------------\n",
    "config_keys = [k for k,v in globals().items() if not k.startswith('_') and isinstance(v, (int, float, bool, str))]\n",
    "#exec(open('configurator.py').read()) # overrides from command line or config file\n",
    "config = {k: globals()[k] for k in config_keys} # will be useful for logging\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# various inits, derived attributes, I/O setup\n",
    "ddp = int(os.environ.get('RANK', -1)) != -1 # is this a ddp run?\n",
    "if ddp:\n",
    "    print('Will use ddp')\n",
    "    init_process_group(backend=backend)\n",
    "    ddp_rank = int(os.environ['RANK'])\n",
    "    ddp_local_rank = int(os.environ['LOCAL_RANK'])\n",
    "    device = f'cuda:{ddp_local_rank}'\n",
    "    torch.cuda.set_device(device)\n",
    "    master_process = ddp_rank == 0 # this process will do logging, checkpointing etc.\n",
    "    seed_offset = ddp_rank # each process gets a different seed\n",
    "else:\n",
    "    print('Will not use ddp')\n",
    "    # if not ddp, we are running on a single gpu, and one process\n",
    "    master_process = True\n",
    "    seed_offset = 0\n",
    "    gradient_accumulation_steps *= 8 # simulate 8 gpus\n",
    "\n",
    "if master_process:\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "torch.manual_seed(1337 + seed_offset)\n",
    "torch.backends.cuda.matmul.allow_tf32 = True # allow tf32 on matmul\n",
    "torch.backends.cudnn.allow_tf32 = True # allow tf32 on cudnn\n",
    "device_type = 'cuda' if 'cuda' in device else 'cpu' # for later use in torch.autocast\n",
    "# note: float16 data type will automatically use a GradScaler\n",
    "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
    "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "data_dir = os.path.join('data', dataset)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "from minlora import LoRAParametrization\n",
    "\n",
    "learning_rate = 1e-3 # use a higher LR for LoRA\n",
    "lora_dropout_p = 0.0\n",
    "rank=4\n",
    "lora_alpha = 64\n",
    "lora_config = {\n",
    "    torch.nn.Embedding: {\n",
    "        \"weight\": partial(LoRAParametrization.from_embedding, rank=rank, lora_alpha=lora_alpha),\n",
    "    },\n",
    "    torch.nn.Linear: {\n",
    "        \"weight\": partial(LoRAParametrization.from_linear, rank=rank, lora_alpha=lora_alpha),\n",
    "    },\n",
    "}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Arzamas\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset generator (/Volumes/WD_RED_RW/machine_learning/cache/generator/default-90a9f236839f8254/0.0.0)\n",
      "Found cached dataset generator (/Volumes/WD_RED_RW/machine_learning/cache/generator/default-311e4af8de1955ef/0.0.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Interfax\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset generator (/Volumes/WD_RED_RW/machine_learning/cache/generator/default-dcdb81eadbeb90a7/0.0.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Lenta\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset generator (/Volumes/WD_RED_RW/machine_learning/cache/generator/default-f2c1883517c39394/0.0.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing NPlus1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset generator (/Volumes/WD_RED_RW/machine_learning/cache/generator/default-5237c7ccbafb679c/0.0.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing proza_ru_2008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset generator (/Volumes/WD_RED_RW/machine_learning/cache/generator/default-0f32410463e05926/0.0.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing proza_ru_2009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset generator (/Volumes/WD_RED_RW/machine_learning/cache/generator/default-0e6b56d2c930469a/0.0.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing proza_ru_2010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset generator (/Volumes/WD_RED_RW/machine_learning/cache/generator/default-23d9d5ca980f03e1/0.0.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing proza_ru_2011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset generator (/Volumes/WD_RED_RW/machine_learning/cache/generator/default-86748e9a76087328/0.0.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing proza_ru_2012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset generator (/Volumes/WD_RED_RW/machine_learning/cache/generator/default-994f4e7eb71caedb/0.0.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing proza_ru_2013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset generator (/Volumes/WD_RED_RW/machine_learning/cache/generator/default-f865b123ee9d7ab6/0.0.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing proza_ru_2014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset generator (/Volumes/WD_RED_RW/machine_learning/cache/generator/default-c323c2be92586bbc/0.0.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing proza_ru_2015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset generator (/Volumes/WD_RED_RW/machine_learning/cache/generator/default-d833c37421182561/0.0.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing proza_ru_2016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset generator (/Volumes/WD_RED_RW/machine_learning/cache/generator/default-377705c522217a49/0.0.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing proza_ru_2017\n"
     ]
    }
   ],
   "source": [
    "cache_dir = '/Volumes/WD_RED_RW/machine_learning/cache'\n",
    "\n",
    "import datasets\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from datasets.builder import DatasetGenerationError\n",
    "\n",
    "# Define the path to the folders containing the text files\n",
    "base = '/Users/seven/Downloads/corpus/'\n",
    "parent_folders = [\n",
    "    \"Arzamas\",\n",
    "    \"Interfax\",\n",
    "    \"Lenta\",\n",
    "    \"NPlus1\",\n",
    "]\n",
    "for year in range(2008, 2018):\n",
    "    parent_folders.append(f\"proza_ru_{year}\" )\n",
    "\n",
    "# Define a function to recursively search for text files\n",
    "def find_text_files(folder_path):\n",
    "    text_files = []\n",
    "    for root, dirs, files in os.walk(folder_path):\n",
    "        for file_name in files:\n",
    "            if file_name.startswith('.'):\n",
    "                continue\n",
    "            if file_name.endswith('.txt'):\n",
    "                file_path = os.path.join(root, file_name)\n",
    "                text_files.append(file_path)\n",
    "        for sub_dir in dirs:\n",
    "            sub_dir_path = os.path.join(root, sub_dir)\n",
    "            text_files += find_text_files(sub_dir_path)\n",
    "    return text_files\n",
    "\n",
    "# Define the function to read text files from a folder\n",
    "def read_files_from_folder(folder_path):\n",
    "    # Find all the text files in the given folder and its subdirectories\n",
    "    text_files = find_text_files(folder_path)\n",
    "\n",
    "    # Iterate through all the text files\n",
    "    for file_path in text_files:\n",
    "        # Read the text from the file\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            text = file.read()\n",
    "        # Yield the text\n",
    "        yield {\"text\": text}\n",
    "\n",
    "# Create the dataset using the read_files_from_folder function\n",
    "datasets_dict = datasets.DatasetDict()\n",
    "for parent_folder in parent_folders:\n",
    "    try:\n",
    "        print(f'Processing {parent_folder}')\n",
    "        folder_path = os.path.join(base, parent_folder)\n",
    "        dataset = datasets.Dataset.from_generator(\n",
    "            generator=lambda: read_files_from_folder(folder_path),\n",
    "            cache_dir=cache_dir\n",
    "            #features=Features.from_dict({\"text\": datasets.Value(\"string\")}),\n",
    "        )\n",
    "        datasets_dict[parent_folder] = dataset\n",
    "    except DatasetGenerationError:\n",
    "        print(f'Will skip {parent_folder}')\n",
    "        continue\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<datasets.iterable_dataset.IterableDataset object at 0x17f6b7c40>\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, interleave_datasets, Dataset, IterableDataset\n",
    "\n",
    "datasets_ru: list[Dataset | IterableDataset] = []\n",
    "for k, v in datasets_dict.items():\n",
    "    v = v.to_iterable_dataset(num_shards=128)\n",
    "    datasets_ru.append(v)\n",
    "print(datasets_ru[0])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "<datasets.iterable_dataset.IterableDataset at 0x28fbc74f0>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset, interleave_datasets\n",
    "\n",
    "# Load data\n",
    "pile = load_dataset(\"EleutherAI/the_pile_deduplicated\", split=\"train\", streaming=True)\n",
    "pile\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "all_datasets = datasets_ru\n",
    "all_datasets.append(pile)\n",
    "multi_dataset = interleave_datasets(all_datasets)\n",
    "\n",
    "shuffled_dataset = multi_dataset.shuffle(buffer_size=10_000, seed=42)\n",
    "\n",
    "train_dataset = shuffled_dataset.skip(1000)\n",
    "validation_dataset = shuffled_dataset.take(1000)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import tiktoken\n",
    "import threading\n",
    "import queue\n",
    "\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "MAX_LENGTH = block_size\n",
    "\n",
    "train_queue = queue.Queue(maxsize=500)\n",
    "validation_queue = queue.Queue(maxsize=500)\n",
    "\n",
    "# Define a function to download and preprocess the data in a background thread\n",
    "def data_worker():\n",
    "    #print(f'data_worker will download')\n",
    "    global train_queue, validation_queue\n",
    "    while True:\n",
    "        # Check if there is space in the data queue\n",
    "        if train_queue.qsize() < train_queue.maxsize:\n",
    "            data = train_dataset.shuffle(buffer_size=1000)\n",
    "            for d in data:\n",
    "                if train_queue.qsize() == train_queue.maxsize:\n",
    "                    break\n",
    "                ids = enc.encode_ordinary(d['text'])\n",
    "                ids.append(enc.eot_token)\n",
    "                train_queue.put(ids)\n",
    "        if validation_queue.qsize() < validation_queue.maxsize:\n",
    "            data = validation_dataset.shuffle(buffer_size=1000)\n",
    "            for d in data:\n",
    "                if validation_queue.qsize() == validation_queue.maxsize:\n",
    "                    break\n",
    "                ids = enc.encode_ordinary(d['text'])\n",
    "                ids.append(enc.eot_token)\n",
    "                validation_queue.put(ids)\n",
    "        else:\n",
    "            # Wait for the batch queue to empty\n",
    "            if train_queue.qsize() == train_queue.maxsize and validation_queue.qsize() == validation_queue.maxsize:\n",
    "                break\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape: torch.Size([3, 1024])\n",
      "y shape: torch.Size([3, 1024])\n"
     ]
    }
   ],
   "source": [
    "def get_batch(split):\n",
    "    #data = train_dataset if split == 'train' else validation_dataset\n",
    "    buffer_ids = []\n",
    "    #print('Will shuffle data')\n",
    "    #data = data.shuffle(buffer_size=1000)\n",
    "\n",
    "    result_ids = []\n",
    "    i = 0\n",
    "    while True:\n",
    "        if len(result_ids) == batch_size:\n",
    "            break\n",
    "        if split == 'train':\n",
    "            if train_queue.empty():\n",
    "                data_worker()\n",
    "            ids = train_queue.get()\n",
    "        else:\n",
    "            if validation_queue.empty():\n",
    "                data_worker()\n",
    "            ids = validation_queue.get()\n",
    "\n",
    "        buffer_ids = buffer_ids + ids\n",
    "\n",
    "        while len(buffer_ids) > MAX_LENGTH + 1:\n",
    "            if len(result_ids) == batch_size:\n",
    "                break\n",
    "            # Truncate text if it's longer than MAX_LENGTH\n",
    "            id = buffer_ids[:MAX_LENGTH + 1]\n",
    "            result_ids.append(id)\n",
    "            buffer_ids = buffer_ids[MAX_LENGTH:]\n",
    "\n",
    "            # Pad text if it's shorter than MAX_LENGTH\n",
    "        i = i + 1\n",
    "\n",
    "    #print(f'train_queue size {train_queue.qsize()}, val: {validation_queue.qsize()}')\n",
    "    #batch_text = [data[i]['text'] for i in range(batch_size)]\n",
    "    x = torch.stack([torch.tensor(ids)[:-1] for ids in result_ids])\n",
    "    y = torch.stack([torch.tensor(ids)[1:] for ids in result_ids])\n",
    "\n",
    "    if device == 'cuda':\n",
    "        # pin arrays x,y, which allows us to move them to GPU asynchronously (non_blocking=True)\n",
    "        x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True)\n",
    "    else:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "# Get a batch of training data\n",
    "x, y = get_batch('train')\n",
    "print(f'x shape: {x.shape}')  # should print \"x shape: torch.Size([12, 1024])\"\n",
    "print(f'y shape: {y.shape}')  # should print \"y shape: torch.Size([12, 1024])\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing a new model from scratch\n",
      "defaulting to vocab_size of GPT-2 to 50304 (50257 rounded up for efficiency)\n",
      "WARNING: using slow attention. Flash Attention atm needs PyTorch nightly and dropout=0.0\n",
      "WARNING: using slow attention. Flash Attention atm needs PyTorch nightly and dropout=0.0\n",
      "WARNING: using slow attention. Flash Attention atm needs PyTorch nightly and dropout=0.0\n",
      "WARNING: using slow attention. Flash Attention atm needs PyTorch nightly and dropout=0.0\n",
      "WARNING: using slow attention. Flash Attention atm needs PyTorch nightly and dropout=0.0\n",
      "WARNING: using slow attention. Flash Attention atm needs PyTorch nightly and dropout=0.0\n",
      "WARNING: using slow attention. Flash Attention atm needs PyTorch nightly and dropout=0.0\n",
      "WARNING: using slow attention. Flash Attention atm needs PyTorch nightly and dropout=0.0\n",
      "WARNING: using slow attention. Flash Attention atm needs PyTorch nightly and dropout=0.0\n",
      "WARNING: using slow attention. Flash Attention atm needs PyTorch nightly and dropout=0.0\n",
      "WARNING: using slow attention. Flash Attention atm needs PyTorch nightly and dropout=0.0\n",
      "WARNING: using slow attention. Flash Attention atm needs PyTorch nightly and dropout=0.0\n",
      "number of parameters: 123.59M\n"
     ]
    },
    {
     "data": {
      "text/plain": "GPT(\n  (transformer): ModuleDict(\n    (wte): ParametrizedEmbedding(\n      50304, 768\n      (parametrizations): ModuleDict(\n        (weight): ParametrizationList(\n          (0): LoRAParametrization()\n        )\n      )\n    )\n    (wpe): ParametrizedEmbedding(\n      1024, 768\n      (parametrizations): ModuleDict(\n        (weight): ParametrizationList(\n          (0): LoRAParametrization()\n        )\n      )\n    )\n    (drop): Dropout(p=0.0, inplace=False)\n    (h): ModuleList(\n      (0): Block(\n        (ln_1): LayerNorm()\n        (attn): CausalSelfAttention(\n          (c_attn): ParametrizedLinear(\n            in_features=768, out_features=2304, bias=False\n            (parametrizations): ModuleDict(\n              (weight): ParametrizationList(\n                (0): LoRAParametrization()\n              )\n            )\n          )\n          (c_proj): ParametrizedLinear(\n            in_features=768, out_features=768, bias=False\n            (parametrizations): ModuleDict(\n              (weight): ParametrizationList(\n                (0): LoRAParametrization()\n              )\n            )\n          )\n          (attn_dropout): Dropout(p=0.0, inplace=False)\n          (resid_dropout): Dropout(p=0.0, inplace=False)\n        )\n        (ln_2): LayerNorm()\n        (mlp): MLP(\n          (c_fc): ParametrizedLinear(\n            in_features=768, out_features=3072, bias=False\n            (parametrizations): ModuleDict(\n              (weight): ParametrizationList(\n                (0): LoRAParametrization()\n              )\n            )\n          )\n          (c_proj): ParametrizedLinear(\n            in_features=3072, out_features=768, bias=False\n            (parametrizations): ModuleDict(\n              (weight): ParametrizationList(\n                (0): LoRAParametrization()\n              )\n            )\n          )\n          (dropout): Dropout(p=0.0, inplace=False)\n        )\n      )\n      (1): Block(\n        (ln_1): LayerNorm()\n        (attn): CausalSelfAttention(\n          (c_attn): ParametrizedLinear(\n            in_features=768, out_features=2304, bias=False\n            (parametrizations): ModuleDict(\n              (weight): ParametrizationList(\n                (0): LoRAParametrization()\n              )\n            )\n          )\n          (c_proj): ParametrizedLinear(\n            in_features=768, out_features=768, bias=False\n            (parametrizations): ModuleDict(\n              (weight): ParametrizationList(\n                (0): LoRAParametrization()\n              )\n            )\n          )\n          (attn_dropout): Dropout(p=0.0, inplace=False)\n          (resid_dropout): Dropout(p=0.0, inplace=False)\n        )\n        (ln_2): LayerNorm()\n        (mlp): MLP(\n          (c_fc): ParametrizedLinear(\n            in_features=768, out_features=3072, bias=False\n            (parametrizations): ModuleDict(\n              (weight): ParametrizationList(\n                (0): LoRAParametrization()\n              )\n            )\n          )\n          (c_proj): ParametrizedLinear(\n            in_features=3072, out_features=768, bias=False\n            (parametrizations): ModuleDict(\n              (weight): ParametrizationList(\n                (0): LoRAParametrization()\n              )\n            )\n          )\n          (dropout): Dropout(p=0.0, inplace=False)\n        )\n      )\n      (2): Block(\n        (ln_1): LayerNorm()\n        (attn): CausalSelfAttention(\n          (c_attn): ParametrizedLinear(\n            in_features=768, out_features=2304, bias=False\n            (parametrizations): ModuleDict(\n              (weight): ParametrizationList(\n                (0): LoRAParametrization()\n              )\n            )\n          )\n          (c_proj): ParametrizedLinear(\n            in_features=768, out_features=768, bias=False\n            (parametrizations): ModuleDict(\n              (weight): ParametrizationList(\n                (0): LoRAParametrization()\n              )\n            )\n          )\n          (attn_dropout): Dropout(p=0.0, inplace=False)\n          (resid_dropout): Dropout(p=0.0, inplace=False)\n        )\n        (ln_2): LayerNorm()\n        (mlp): MLP(\n          (c_fc): ParametrizedLinear(\n            in_features=768, out_features=3072, bias=False\n            (parametrizations): ModuleDict(\n              (weight): ParametrizationList(\n                (0): LoRAParametrization()\n              )\n            )\n          )\n          (c_proj): ParametrizedLinear(\n            in_features=3072, out_features=768, bias=False\n            (parametrizations): ModuleDict(\n              (weight): ParametrizationList(\n                (0): LoRAParametrization()\n              )\n            )\n          )\n          (dropout): Dropout(p=0.0, inplace=False)\n        )\n      )\n      (3): Block(\n        (ln_1): LayerNorm()\n        (attn): CausalSelfAttention(\n          (c_attn): ParametrizedLinear(\n            in_features=768, out_features=2304, bias=False\n            (parametrizations): ModuleDict(\n              (weight): ParametrizationList(\n                (0): LoRAParametrization()\n              )\n            )\n          )\n          (c_proj): ParametrizedLinear(\n            in_features=768, out_features=768, bias=False\n            (parametrizations): ModuleDict(\n              (weight): ParametrizationList(\n                (0): LoRAParametrization()\n              )\n            )\n          )\n          (attn_dropout): Dropout(p=0.0, inplace=False)\n          (resid_dropout): Dropout(p=0.0, inplace=False)\n        )\n        (ln_2): LayerNorm()\n        (mlp): MLP(\n          (c_fc): ParametrizedLinear(\n            in_features=768, out_features=3072, bias=False\n            (parametrizations): ModuleDict(\n              (weight): ParametrizationList(\n                (0): LoRAParametrization()\n              )\n            )\n          )\n          (c_proj): ParametrizedLinear(\n            in_features=3072, out_features=768, bias=False\n            (parametrizations): ModuleDict(\n              (weight): ParametrizationList(\n                (0): LoRAParametrization()\n              )\n            )\n          )\n          (dropout): Dropout(p=0.0, inplace=False)\n        )\n      )\n      (4): Block(\n        (ln_1): LayerNorm()\n        (attn): CausalSelfAttention(\n          (c_attn): ParametrizedLinear(\n            in_features=768, out_features=2304, bias=False\n            (parametrizations): ModuleDict(\n              (weight): ParametrizationList(\n                (0): LoRAParametrization()\n              )\n            )\n          )\n          (c_proj): ParametrizedLinear(\n            in_features=768, out_features=768, bias=False\n            (parametrizations): ModuleDict(\n              (weight): ParametrizationList(\n                (0): LoRAParametrization()\n              )\n            )\n          )\n          (attn_dropout): Dropout(p=0.0, inplace=False)\n          (resid_dropout): Dropout(p=0.0, inplace=False)\n        )\n        (ln_2): LayerNorm()\n        (mlp): MLP(\n          (c_fc): ParametrizedLinear(\n            in_features=768, out_features=3072, bias=False\n            (parametrizations): ModuleDict(\n              (weight): ParametrizationList(\n                (0): LoRAParametrization()\n              )\n            )\n          )\n          (c_proj): ParametrizedLinear(\n            in_features=3072, out_features=768, bias=False\n            (parametrizations): ModuleDict(\n              (weight): ParametrizationList(\n                (0): LoRAParametrization()\n              )\n            )\n          )\n          (dropout): Dropout(p=0.0, inplace=False)\n        )\n      )\n      (5): Block(\n        (ln_1): LayerNorm()\n        (attn): CausalSelfAttention(\n          (c_attn): ParametrizedLinear(\n            in_features=768, out_features=2304, bias=False\n            (parametrizations): ModuleDict(\n              (weight): ParametrizationList(\n                (0): LoRAParametrization()\n              )\n            )\n          )\n          (c_proj): ParametrizedLinear(\n            in_features=768, out_features=768, bias=False\n            (parametrizations): ModuleDict(\n              (weight): ParametrizationList(\n                (0): LoRAParametrization()\n              )\n            )\n          )\n          (attn_dropout): Dropout(p=0.0, inplace=False)\n          (resid_dropout): Dropout(p=0.0, inplace=False)\n        )\n        (ln_2): LayerNorm()\n        (mlp): MLP(\n          (c_fc): ParametrizedLinear(\n            in_features=768, out_features=3072, bias=False\n            (parametrizations): ModuleDict(\n              (weight): ParametrizationList(\n                (0): LoRAParametrization()\n              )\n            )\n          )\n          (c_proj): ParametrizedLinear(\n            in_features=3072, out_features=768, bias=False\n            (parametrizations): ModuleDict(\n              (weight): ParametrizationList(\n                (0): LoRAParametrization()\n              )\n            )\n          )\n          (dropout): Dropout(p=0.0, inplace=False)\n        )\n      )\n      (6): Block(\n        (ln_1): LayerNorm()\n        (attn): CausalSelfAttention(\n          (c_attn): ParametrizedLinear(\n            in_features=768, out_features=2304, bias=False\n            (parametrizations): ModuleDict(\n              (weight): ParametrizationList(\n                (0): LoRAParametrization()\n              )\n            )\n          )\n          (c_proj): ParametrizedLinear(\n            in_features=768, out_features=768, bias=False\n            (parametrizations): ModuleDict(\n              (weight): ParametrizationList(\n                (0): LoRAParametrization()\n              )\n            )\n          )\n          (attn_dropout): Dropout(p=0.0, inplace=False)\n          (resid_dropout): Dropout(p=0.0, inplace=False)\n        )\n        (ln_2): LayerNorm()\n        (mlp): MLP(\n          (c_fc): ParametrizedLinear(\n            in_features=768, out_features=3072, bias=False\n            (parametrizations): ModuleDict(\n              (weight): ParametrizationList(\n                (0): LoRAParametrization()\n              )\n            )\n          )\n          (c_proj): ParametrizedLinear(\n            in_features=3072, out_features=768, bias=False\n            (parametrizations): ModuleDict(\n              (weight): ParametrizationList(\n                (0): LoRAParametrization()\n              )\n            )\n          )\n          (dropout): Dropout(p=0.0, inplace=False)\n        )\n      )\n      (7): Block(\n        (ln_1): LayerNorm()\n        (attn): CausalSelfAttention(\n          (c_attn): ParametrizedLinear(\n            in_features=768, out_features=2304, bias=False\n            (parametrizations): ModuleDict(\n              (weight): ParametrizationList(\n                (0): LoRAParametrization()\n              )\n            )\n          )\n          (c_proj): ParametrizedLinear(\n            in_features=768, out_features=768, bias=False\n            (parametrizations): ModuleDict(\n              (weight): ParametrizationList(\n                (0): LoRAParametrization()\n              )\n            )\n          )\n          (attn_dropout): Dropout(p=0.0, inplace=False)\n          (resid_dropout): Dropout(p=0.0, inplace=False)\n        )\n        (ln_2): LayerNorm()\n        (mlp): MLP(\n          (c_fc): ParametrizedLinear(\n            in_features=768, out_features=3072, bias=False\n            (parametrizations): ModuleDict(\n              (weight): ParametrizationList(\n                (0): LoRAParametrization()\n              )\n            )\n          )\n          (c_proj): ParametrizedLinear(\n            in_features=3072, out_features=768, bias=False\n            (parametrizations): ModuleDict(\n              (weight): ParametrizationList(\n                (0): LoRAParametrization()\n              )\n            )\n          )\n          (dropout): Dropout(p=0.0, inplace=False)\n        )\n      )\n      (8): Block(\n        (ln_1): LayerNorm()\n        (attn): CausalSelfAttention(\n          (c_attn): ParametrizedLinear(\n            in_features=768, out_features=2304, bias=False\n            (parametrizations): ModuleDict(\n              (weight): ParametrizationList(\n                (0): LoRAParametrization()\n              )\n            )\n          )\n          (c_proj): ParametrizedLinear(\n            in_features=768, out_features=768, bias=False\n            (parametrizations): ModuleDict(\n              (weight): ParametrizationList(\n                (0): LoRAParametrization()\n              )\n            )\n          )\n          (attn_dropout): Dropout(p=0.0, inplace=False)\n          (resid_dropout): Dropout(p=0.0, inplace=False)\n        )\n        (ln_2): LayerNorm()\n        (mlp): MLP(\n          (c_fc): ParametrizedLinear(\n            in_features=768, out_features=3072, bias=False\n            (parametrizations): ModuleDict(\n              (weight): ParametrizationList(\n                (0): LoRAParametrization()\n              )\n            )\n          )\n          (c_proj): ParametrizedLinear(\n            in_features=3072, out_features=768, bias=False\n            (parametrizations): ModuleDict(\n              (weight): ParametrizationList(\n                (0): LoRAParametrization()\n              )\n            )\n          )\n          (dropout): Dropout(p=0.0, inplace=False)\n        )\n      )\n      (9): Block(\n        (ln_1): LayerNorm()\n        (attn): CausalSelfAttention(\n          (c_attn): ParametrizedLinear(\n            in_features=768, out_features=2304, bias=False\n            (parametrizations): ModuleDict(\n              (weight): ParametrizationList(\n                (0): LoRAParametrization()\n              )\n            )\n          )\n          (c_proj): ParametrizedLinear(\n            in_features=768, out_features=768, bias=False\n            (parametrizations): ModuleDict(\n              (weight): ParametrizationList(\n                (0): LoRAParametrization()\n              )\n            )\n          )\n          (attn_dropout): Dropout(p=0.0, inplace=False)\n          (resid_dropout): Dropout(p=0.0, inplace=False)\n        )\n        (ln_2): LayerNorm()\n        (mlp): MLP(\n          (c_fc): ParametrizedLinear(\n            in_features=768, out_features=3072, bias=False\n            (parametrizations): ModuleDict(\n              (weight): ParametrizationList(\n                (0): LoRAParametrization()\n              )\n            )\n          )\n          (c_proj): ParametrizedLinear(\n            in_features=3072, out_features=768, bias=False\n            (parametrizations): ModuleDict(\n              (weight): ParametrizationList(\n                (0): LoRAParametrization()\n              )\n            )\n          )\n          (dropout): Dropout(p=0.0, inplace=False)\n        )\n      )\n      (10): Block(\n        (ln_1): LayerNorm()\n        (attn): CausalSelfAttention(\n          (c_attn): ParametrizedLinear(\n            in_features=768, out_features=2304, bias=False\n            (parametrizations): ModuleDict(\n              (weight): ParametrizationList(\n                (0): LoRAParametrization()\n              )\n            )\n          )\n          (c_proj): ParametrizedLinear(\n            in_features=768, out_features=768, bias=False\n            (parametrizations): ModuleDict(\n              (weight): ParametrizationList(\n                (0): LoRAParametrization()\n              )\n            )\n          )\n          (attn_dropout): Dropout(p=0.0, inplace=False)\n          (resid_dropout): Dropout(p=0.0, inplace=False)\n        )\n        (ln_2): LayerNorm()\n        (mlp): MLP(\n          (c_fc): ParametrizedLinear(\n            in_features=768, out_features=3072, bias=False\n            (parametrizations): ModuleDict(\n              (weight): ParametrizationList(\n                (0): LoRAParametrization()\n              )\n            )\n          )\n          (c_proj): ParametrizedLinear(\n            in_features=3072, out_features=768, bias=False\n            (parametrizations): ModuleDict(\n              (weight): ParametrizationList(\n                (0): LoRAParametrization()\n              )\n            )\n          )\n          (dropout): Dropout(p=0.0, inplace=False)\n        )\n      )\n      (11): Block(\n        (ln_1): LayerNorm()\n        (attn): CausalSelfAttention(\n          (c_attn): ParametrizedLinear(\n            in_features=768, out_features=2304, bias=False\n            (parametrizations): ModuleDict(\n              (weight): ParametrizationList(\n                (0): LoRAParametrization()\n              )\n            )\n          )\n          (c_proj): ParametrizedLinear(\n            in_features=768, out_features=768, bias=False\n            (parametrizations): ModuleDict(\n              (weight): ParametrizationList(\n                (0): LoRAParametrization()\n              )\n            )\n          )\n          (attn_dropout): Dropout(p=0.0, inplace=False)\n          (resid_dropout): Dropout(p=0.0, inplace=False)\n        )\n        (ln_2): LayerNorm()\n        (mlp): MLP(\n          (c_fc): ParametrizedLinear(\n            in_features=768, out_features=3072, bias=False\n            (parametrizations): ModuleDict(\n              (weight): ParametrizationList(\n                (0): LoRAParametrization()\n              )\n            )\n          )\n          (c_proj): ParametrizedLinear(\n            in_features=3072, out_features=768, bias=False\n            (parametrizations): ModuleDict(\n              (weight): ParametrizationList(\n                (0): LoRAParametrization()\n              )\n            )\n          )\n          (dropout): Dropout(p=0.0, inplace=False)\n        )\n      )\n    )\n    (ln_f): LayerNorm()\n  )\n  (lm_head): ParametrizedLinear(\n    in_features=768, out_features=50304, bias=False\n    (parametrizations): ModuleDict(\n      (weight): ParametrizationList(\n        (0): LoRAParametrization()\n      )\n    )\n  )\n)"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# init these up here, can override if init_from='resume' (i.e. from a checkpoint)\n",
    "iter_num = 0\n",
    "best_val_loss = 1e9\n",
    "\n",
    "# attempt to derive vocab_size from the dataset\n",
    "meta_path = os.path.join(data_dir, 'meta.pkl')\n",
    "meta_vocab_size = None\n",
    "if os.path.exists(meta_path):\n",
    "    with open(meta_path, 'rb') as f:\n",
    "        meta = pickle.load(f)\n",
    "    meta_vocab_size = meta['vocab_size']\n",
    "    print(f\"found vocab_size = {meta_vocab_size} (inside {meta_path})\")\n",
    "\n",
    "# model init\n",
    "model_args = dict(n_layer=n_layer, n_head=n_head, n_embd=n_embd, block_size=block_size,\n",
    "                  bias=bias, vocab_size=None, dropout=dropout) # start with model_args from command line\n",
    "if init_from == 'scratch':\n",
    "    # init a new model from scratch\n",
    "    print(\"Initializing a new model from scratch\")\n",
    "    # determine the vocab size we'll use for from-scratch training\n",
    "    if meta_vocab_size is None:\n",
    "        print(\"defaulting to vocab_size of GPT-2 to 50304 (50257 rounded up for efficiency)\")\n",
    "    model_args['vocab_size'] = meta_vocab_size if meta_vocab_size is not None else 50304\n",
    "    gptconf = GPTConfig(**model_args)\n",
    "    model = GPT(gptconf)\n",
    "elif init_from == 'resume':\n",
    "    print(f\"Resuming training from {out_dir}\")\n",
    "    # resume training from a checkpoint.\n",
    "    ckpt_path = os.path.join(out_dir, 'ckpt.pt')\n",
    "    checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "    checkpoint_model_args = checkpoint['model_args']\n",
    "    # force these config attributes to be equal otherwise we can't even resume training\n",
    "    # the rest of the attributes (e.g. dropout) can stay as desired from command line\n",
    "    for k in ['n_layer', 'n_head', 'n_embd', 'block_size', 'bias', 'vocab_size']:\n",
    "        model_args[k] = checkpoint_model_args[k]\n",
    "    # create the model\n",
    "    gptconf = GPTConfig(**model_args)\n",
    "    model = GPT(gptconf)\n",
    "    state_dict = checkpoint['model']\n",
    "    # fix the keys of the state dictionary :(\n",
    "    # honestly no idea how checkpoints sometimes get this prefix, have to debug more\n",
    "    unwanted_prefix = '_orig_mod.'\n",
    "    for k,v in list(state_dict.items()):\n",
    "        if k.startswith(unwanted_prefix):\n",
    "            state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
    "    model.load_state_dict(state_dict)\n",
    "    iter_num = checkpoint['iter_num']\n",
    "    best_val_loss = checkpoint['best_val_loss']\n",
    "elif init_from.startswith('gpt2'):\n",
    "    print(f\"Initializing from OpenAI GPT-2 weights: {init_from}\")\n",
    "    # initialize from OpenAI GPT-2 weights\n",
    "    override_args = dict(dropout=dropout)\n",
    "    model = GPT.from_pretrained(init_from, override_args)\n",
    "    # read off the created config params, so we can store them into checkpoint correctly\n",
    "    for k in ['n_layer', 'n_head', 'n_embd', 'block_size', 'bias', 'vocab_size']:\n",
    "        model_args[k] = getattr(model.config, k)\n",
    "# crop down the model block size if desired, using model surgery\n",
    "if block_size < model.config.block_size:\n",
    "    model.crop_block_size(block_size)\n",
    "    model_args['block_size'] = block_size # so that the checkpoint will have the right value\n",
    "if use_lora:\n",
    "    minlora.add_lora(model, lora_config=lora_config)\n",
    "    minlora.tie_weights(linear=model.lm_head, embedding=model.transformer.wte)\n",
    "model.to(device)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimizing 801.3k parameters\n",
      "using fused AdamW: False\n"
     ]
    }
   ],
   "source": [
    "scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
    "\n",
    "# optimizer\n",
    "def configure_optimizers_lora(self, weight_decay, learning_rate, betas, device_type):\n",
    "    # we apply weight decay to all lora params\n",
    "    optim_groups = [\n",
    "        # note: .get_lora_params() returns a generator\n",
    "        # we need to wrap it in a list so we can consume it twice\n",
    "        {\"params\": list(minlora.get_lora_params(self)) , \"weight_decay\": weight_decay},\n",
    "        # you can also add biases for fine-tuning,\n",
    "        # but I want to make sure lora alone works\n",
    "        # {\"params\": minlora.get_bias_params(self), \"weight_decay\": 0.0}, # bias params don't get weight decay\n",
    "    ]\n",
    "\n",
    "    def parameter_count(optim_groups):\n",
    "        n = sum(p.numel() for group in optim_groups for p in group[\"params\"])\n",
    "        if n < 1e6:\n",
    "            return f\"{n/1e3:.1f}k\"\n",
    "        else:\n",
    "            return f\"{n/1e6:.1f}M\"\n",
    "\n",
    "    print(f\"optimizing {parameter_count(optim_groups)} parameters\")\n",
    "\n",
    "    # new PyTorch nightly has a new 'fused' option for AdamW that is much faster\n",
    "    use_fused = (device_type == \"cuda\") and (\"fused\" in inspect.signature(torch.optim.AdamW).parameters)\n",
    "    print(f\"using fused AdamW: {use_fused}\")\n",
    "    extra_args = dict(fused=True) if use_fused else dict()\n",
    "    optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=betas, **extra_args)\n",
    "\n",
    "    return optimizer\n",
    "if use_lora:\n",
    "    optimizer = configure_optimizers_lora(model, weight_decay, learning_rate, (beta1, beta2), device_type)\n",
    "else:\n",
    "    optimizer = model.configure_optimizers(weight_decay, learning_rate, (beta1, beta2), device_type)\n",
    "if init_from == 'resume':\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "\n",
    "# compile the model\n",
    "if compile:\n",
    "    print(\"compiling the model... (takes a ~minute)\")\n",
    "    unoptimized_model = model\n",
    "    model = torch.compile(model) # requires PyTorch 2.0\n",
    "\n",
    "# wrap model into DDP container\n",
    "if ddp:\n",
    "    model = DDP(model, device_ids=[ddp_local_rank])\n",
    "\n",
    "# helps estimate an arbitrarily accurate loss over either split using many batches\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            with ctx:\n",
    "                logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "# learning rate decay scheduler (cosine with warmup)\n",
    "def get_lr(it):\n",
    "    # 1) linear warmup for warmup_iters steps\n",
    "    if it < warmup_iters:\n",
    "        return learning_rate * it / warmup_iters\n",
    "    # 2) if it > lr_decay_iters, return min learning rate\n",
    "    if it > lr_decay_iters:\n",
    "        return min_lr\n",
    "    # 3) in between, use cosine decay down to min learning rate\n",
    "    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n",
    "    assert 0 <= decay_ratio <= 1\n",
    "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff ranges 0..1\n",
    "    return min_lr + coeff * (learning_rate - min_lr)\n",
    "\n",
    "# logging\n",
    "if wandb_log and master_process:\n",
    "    import wandb\n",
    "    wandb.init(project=wandb_project, name=wandb_run_name, config=config)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "# training loop\n",
    "X, Y = get_batch('train') # fetch the very first batch\n",
    "t0 = time.time()\n",
    "local_iter_num = 0 # number of iterations in the lifetime of this process\n",
    "raw_model = model.module if ddp else model # unwrap DDP container if needed\n",
    "running_mfu = -1.0"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "GPT(\n  (transformer): ModuleDict(\n    (wte): ParametrizedEmbedding(\n      50304, 768\n      (parametrizations): ModuleDict(\n        (weight): ParametrizationList(\n          (0): LoRAParametrization()\n        )\n      )\n    )\n    (wpe): ParametrizedEmbedding(\n      1024, 768\n      (parametrizations): ModuleDict(\n        (weight): ParametrizationList(\n          (0): LoRAParametrization()\n        )\n      )\n    )\n    (drop): Dropout(p=0.0, inplace=False)\n    (h): ModuleList(\n      (0): Block(\n        (ln_1): LayerNorm()\n        (attn): CausalSelfAttention(\n          (c_attn): ParametrizedLinear(\n            in_features=768, out_features=2304, bias=False\n            (parametrizations): ModuleDict(\n              (weight): ParametrizationList(\n                (0): LoRAParametrization()\n              )\n            )\n          )\n          (c_proj): ParametrizedLinear(\n            in_features=768, out_features=768, bias=False\n            (parametrizations): ModuleDict(\n              (weight): ParametrizationList(\n                (0): LoRAParametrization()\n              )\n            )\n          )\n          (attn_dropout): Dropout(p=0.0, inplace=False)\n          (resid_dropout): Dropout(p=0.0, inplace=False)\n        )\n        (ln_2): LayerNorm()\n        (mlp): MLP(\n          (c_fc): ParametrizedLinear(\n            in_features=768, out_features=3072, bias=False\n            (parametrizations): ModuleDict(\n              (weight): ParametrizationList(\n                (0): LoRAParametrization()\n              )\n            )\n          )\n          (c_proj): ParametrizedLinear(\n            in_features=3072, out_features=768, bias=False\n            (parametrizations): ModuleDict(\n              (weight): ParametrizationList(\n                (0): LoRAParametrization()\n              )\n            )\n          )\n          (dropout): Dropout(p=0.0, inplace=False)\n        )\n      )\n      (1): Block(\n        (ln_1): LayerNorm()\n        (attn): CausalSelfAttention(\n          (c_attn): ParametrizedLinear(\n            in_features=768, out_features=2304, bias=False\n            (parametrizations): ModuleDict(\n              (weight): ParametrizationList(\n                (0): LoRAParametrization()\n              )\n            )\n          )\n          (c_proj): ParametrizedLinear(\n            in_features=768, out_features=768, bias=False\n            (parametrizations): ModuleDict(\n              (weight): ParametrizationList(\n                (0): LoRAParametrization()\n              )\n            )\n          )\n          (attn_dropout): Dropout(p=0.0, inplace=False)\n          (resid_dropout): Dropout(p=0.0, inplace=False)\n        )\n        (ln_2): LayerNorm()\n        (mlp): MLP(\n          (c_fc): ParametrizedLinear(\n            in_features=768, out_features=3072, bias=False\n            (parametrizations): ModuleDict(\n              (weight): ParametrizationList(\n                (0): LoRAParametrization()\n              )\n            )\n          )\n          (c_proj): ParametrizedLinear(\n            in_features=3072, out_features=768, bias=False\n            (parametrizations): ModuleDict(\n              (weight): ParametrizationList(\n                (0): LoRAParametrization()\n              )\n            )\n          )\n          (dropout): Dropout(p=0.0, inplace=False)\n        )\n      )\n      (2): Block(\n        (ln_1): LayerNorm()\n        (attn): CausalSelfAttention(\n          (c_attn): ParametrizedLinear(\n            in_features=768, out_features=2304, bias=False\n            (parametrizations): ModuleDict(\n              (weight): ParametrizationList(\n                (0): LoRAParametrization()\n              )\n            )\n          )\n          (c_proj): ParametrizedLinear(\n            in_features=768, out_features=768, bias=False\n            (parametrizations): ModuleDict(\n              (weight): ParametrizationList(\n                (0): LoRAParametrization()\n              )\n            )\n          )\n          (attn_dropout): Dropout(p=0.0, inplace=False)\n          (resid_dropout): Dropout(p=0.0, inplace=False)\n        )\n        (ln_2): LayerNorm()\n        (mlp): MLP(\n          (c_fc): ParametrizedLinear(\n            in_features=768, out_features=3072, bias=False\n            (parametrizations): ModuleDict(\n              (weight): ParametrizationList(\n                (0): LoRAParametrization()\n              )\n            )\n          )\n          (c_proj): ParametrizedLinear(\n            in_features=3072, out_features=768, bias=False\n            (parametrizations): ModuleDict(\n              (weight): ParametrizationList(\n                (0): LoRAParametrization()\n              )\n            )\n          )\n          (dropout): Dropout(p=0.0, inplace=False)\n        )\n      )\n      (3): Block(\n        (ln_1): LayerNorm()\n        (attn): CausalSelfAttention(\n          (c_attn): ParametrizedLinear(\n            in_features=768, out_features=2304, bias=False\n            (parametrizations): ModuleDict(\n              (weight): ParametrizationList(\n                (0): LoRAParametrization()\n              )\n            )\n          )\n          (c_proj): ParametrizedLinear(\n            in_features=768, out_features=768, bias=False\n            (parametrizations): ModuleDict(\n              (weight): ParametrizationList(\n                (0): LoRAParametrization()\n              )\n            )\n          )\n          (attn_dropout): Dropout(p=0.0, inplace=False)\n          (resid_dropout): Dropout(p=0.0, inplace=False)\n        )\n        (ln_2): LayerNorm()\n        (mlp): MLP(\n          (c_fc): ParametrizedLinear(\n            in_features=768, out_features=3072, bias=False\n            (parametrizations): ModuleDict(\n              (weight): ParametrizationList(\n                (0): LoRAParametrization()\n              )\n            )\n          )\n          (c_proj): ParametrizedLinear(\n            in_features=3072, out_features=768, bias=False\n            (parametrizations): ModuleDict(\n              (weight): ParametrizationList(\n                (0): LoRAParametrization()\n              )\n            )\n          )\n          (dropout): Dropout(p=0.0, inplace=False)\n        )\n      )\n      (4): Block(\n        (ln_1): LayerNorm()\n        (attn): CausalSelfAttention(\n          (c_attn): ParametrizedLinear(\n            in_features=768, out_features=2304, bias=False\n            (parametrizations): ModuleDict(\n              (weight): ParametrizationList(\n                (0): LoRAParametrization()\n              )\n            )\n          )\n          (c_proj): ParametrizedLinear(\n            in_features=768, out_features=768, bias=False\n            (parametrizations): ModuleDict(\n              (weight): ParametrizationList(\n                (0): LoRAParametrization()\n              )\n            )\n          )\n          (attn_dropout): Dropout(p=0.0, inplace=False)\n          (resid_dropout): Dropout(p=0.0, inplace=False)\n        )\n        (ln_2): LayerNorm()\n        (mlp): MLP(\n          (c_fc): ParametrizedLinear(\n            in_features=768, out_features=3072, bias=False\n            (parametrizations): ModuleDict(\n              (weight): ParametrizationList(\n                (0): LoRAParametrization()\n              )\n            )\n          )\n          (c_proj): ParametrizedLinear(\n            in_features=3072, out_features=768, bias=False\n            (parametrizations): ModuleDict(\n              (weight): ParametrizationList(\n                (0): LoRAParametrization()\n              )\n            )\n          )\n          (dropout): Dropout(p=0.0, inplace=False)\n        )\n      )\n      (5): Block(\n        (ln_1): LayerNorm()\n        (attn): CausalSelfAttention(\n          (c_attn): ParametrizedLinear(\n            in_features=768, out_features=2304, bias=False\n            (parametrizations): ModuleDict(\n              (weight): ParametrizationList(\n                (0): LoRAParametrization()\n              )\n            )\n          )\n          (c_proj): ParametrizedLinear(\n            in_features=768, out_features=768, bias=False\n            (parametrizations): ModuleDict(\n              (weight): ParametrizationList(\n                (0): LoRAParametrization()\n              )\n            )\n          )\n          (attn_dropout): Dropout(p=0.0, inplace=False)\n          (resid_dropout): Dropout(p=0.0, inplace=False)\n        )\n        (ln_2): LayerNorm()\n        (mlp): MLP(\n          (c_fc): ParametrizedLinear(\n            in_features=768, out_features=3072, bias=False\n            (parametrizations): ModuleDict(\n              (weight): ParametrizationList(\n                (0): LoRAParametrization()\n              )\n            )\n          )\n          (c_proj): ParametrizedLinear(\n            in_features=3072, out_features=768, bias=False\n            (parametrizations): ModuleDict(\n              (weight): ParametrizationList(\n                (0): LoRAParametrization()\n              )\n            )\n          )\n          (dropout): Dropout(p=0.0, inplace=False)\n        )\n      )\n      (6): Block(\n        (ln_1): LayerNorm()\n        (attn): CausalSelfAttention(\n          (c_attn): ParametrizedLinear(\n            in_features=768, out_features=2304, bias=False\n            (parametrizations): ModuleDict(\n              (weight): ParametrizationList(\n                (0): LoRAParametrization()\n              )\n            )\n          )\n          (c_proj): ParametrizedLinear(\n            in_features=768, out_features=768, bias=False\n            (parametrizations): ModuleDict(\n              (weight): ParametrizationList(\n                (0): LoRAParametrization()\n              )\n            )\n          )\n          (attn_dropout): Dropout(p=0.0, inplace=False)\n          (resid_dropout): Dropout(p=0.0, inplace=False)\n        )\n        (ln_2): LayerNorm()\n        (mlp): MLP(\n          (c_fc): ParametrizedLinear(\n            in_features=768, out_features=3072, bias=False\n            (parametrizations): ModuleDict(\n              (weight): ParametrizationList(\n                (0): LoRAParametrization()\n              )\n            )\n          )\n          (c_proj): ParametrizedLinear(\n            in_features=3072, out_features=768, bias=False\n            (parametrizations): ModuleDict(\n              (weight): ParametrizationList(\n                (0): LoRAParametrization()\n              )\n            )\n          )\n          (dropout): Dropout(p=0.0, inplace=False)\n        )\n      )\n      (7): Block(\n        (ln_1): LayerNorm()\n        (attn): CausalSelfAttention(\n          (c_attn): ParametrizedLinear(\n            in_features=768, out_features=2304, bias=False\n            (parametrizations): ModuleDict(\n              (weight): ParametrizationList(\n                (0): LoRAParametrization()\n              )\n            )\n          )\n          (c_proj): ParametrizedLinear(\n            in_features=768, out_features=768, bias=False\n            (parametrizations): ModuleDict(\n              (weight): ParametrizationList(\n                (0): LoRAParametrization()\n              )\n            )\n          )\n          (attn_dropout): Dropout(p=0.0, inplace=False)\n          (resid_dropout): Dropout(p=0.0, inplace=False)\n        )\n        (ln_2): LayerNorm()\n        (mlp): MLP(\n          (c_fc): ParametrizedLinear(\n            in_features=768, out_features=3072, bias=False\n            (parametrizations): ModuleDict(\n              (weight): ParametrizationList(\n                (0): LoRAParametrization()\n              )\n            )\n          )\n          (c_proj): ParametrizedLinear(\n            in_features=3072, out_features=768, bias=False\n            (parametrizations): ModuleDict(\n              (weight): ParametrizationList(\n                (0): LoRAParametrization()\n              )\n            )\n          )\n          (dropout): Dropout(p=0.0, inplace=False)\n        )\n      )\n      (8): Block(\n        (ln_1): LayerNorm()\n        (attn): CausalSelfAttention(\n          (c_attn): ParametrizedLinear(\n            in_features=768, out_features=2304, bias=False\n            (parametrizations): ModuleDict(\n              (weight): ParametrizationList(\n                (0): LoRAParametrization()\n              )\n            )\n          )\n          (c_proj): ParametrizedLinear(\n            in_features=768, out_features=768, bias=False\n            (parametrizations): ModuleDict(\n              (weight): ParametrizationList(\n                (0): LoRAParametrization()\n              )\n            )\n          )\n          (attn_dropout): Dropout(p=0.0, inplace=False)\n          (resid_dropout): Dropout(p=0.0, inplace=False)\n        )\n        (ln_2): LayerNorm()\n        (mlp): MLP(\n          (c_fc): ParametrizedLinear(\n            in_features=768, out_features=3072, bias=False\n            (parametrizations): ModuleDict(\n              (weight): ParametrizationList(\n                (0): LoRAParametrization()\n              )\n            )\n          )\n          (c_proj): ParametrizedLinear(\n            in_features=3072, out_features=768, bias=False\n            (parametrizations): ModuleDict(\n              (weight): ParametrizationList(\n                (0): LoRAParametrization()\n              )\n            )\n          )\n          (dropout): Dropout(p=0.0, inplace=False)\n        )\n      )\n      (9): Block(\n        (ln_1): LayerNorm()\n        (attn): CausalSelfAttention(\n          (c_attn): ParametrizedLinear(\n            in_features=768, out_features=2304, bias=False\n            (parametrizations): ModuleDict(\n              (weight): ParametrizationList(\n                (0): LoRAParametrization()\n              )\n            )\n          )\n          (c_proj): ParametrizedLinear(\n            in_features=768, out_features=768, bias=False\n            (parametrizations): ModuleDict(\n              (weight): ParametrizationList(\n                (0): LoRAParametrization()\n              )\n            )\n          )\n          (attn_dropout): Dropout(p=0.0, inplace=False)\n          (resid_dropout): Dropout(p=0.0, inplace=False)\n        )\n        (ln_2): LayerNorm()\n        (mlp): MLP(\n          (c_fc): ParametrizedLinear(\n            in_features=768, out_features=3072, bias=False\n            (parametrizations): ModuleDict(\n              (weight): ParametrizationList(\n                (0): LoRAParametrization()\n              )\n            )\n          )\n          (c_proj): ParametrizedLinear(\n            in_features=3072, out_features=768, bias=False\n            (parametrizations): ModuleDict(\n              (weight): ParametrizationList(\n                (0): LoRAParametrization()\n              )\n            )\n          )\n          (dropout): Dropout(p=0.0, inplace=False)\n        )\n      )\n      (10): Block(\n        (ln_1): LayerNorm()\n        (attn): CausalSelfAttention(\n          (c_attn): ParametrizedLinear(\n            in_features=768, out_features=2304, bias=False\n            (parametrizations): ModuleDict(\n              (weight): ParametrizationList(\n                (0): LoRAParametrization()\n              )\n            )\n          )\n          (c_proj): ParametrizedLinear(\n            in_features=768, out_features=768, bias=False\n            (parametrizations): ModuleDict(\n              (weight): ParametrizationList(\n                (0): LoRAParametrization()\n              )\n            )\n          )\n          (attn_dropout): Dropout(p=0.0, inplace=False)\n          (resid_dropout): Dropout(p=0.0, inplace=False)\n        )\n        (ln_2): LayerNorm()\n        (mlp): MLP(\n          (c_fc): ParametrizedLinear(\n            in_features=768, out_features=3072, bias=False\n            (parametrizations): ModuleDict(\n              (weight): ParametrizationList(\n                (0): LoRAParametrization()\n              )\n            )\n          )\n          (c_proj): ParametrizedLinear(\n            in_features=3072, out_features=768, bias=False\n            (parametrizations): ModuleDict(\n              (weight): ParametrizationList(\n                (0): LoRAParametrization()\n              )\n            )\n          )\n          (dropout): Dropout(p=0.0, inplace=False)\n        )\n      )\n      (11): Block(\n        (ln_1): LayerNorm()\n        (attn): CausalSelfAttention(\n          (c_attn): ParametrizedLinear(\n            in_features=768, out_features=2304, bias=False\n            (parametrizations): ModuleDict(\n              (weight): ParametrizationList(\n                (0): LoRAParametrization()\n              )\n            )\n          )\n          (c_proj): ParametrizedLinear(\n            in_features=768, out_features=768, bias=False\n            (parametrizations): ModuleDict(\n              (weight): ParametrizationList(\n                (0): LoRAParametrization()\n              )\n            )\n          )\n          (attn_dropout): Dropout(p=0.0, inplace=False)\n          (resid_dropout): Dropout(p=0.0, inplace=False)\n        )\n        (ln_2): LayerNorm()\n        (mlp): MLP(\n          (c_fc): ParametrizedLinear(\n            in_features=768, out_features=3072, bias=False\n            (parametrizations): ModuleDict(\n              (weight): ParametrizationList(\n                (0): LoRAParametrization()\n              )\n            )\n          )\n          (c_proj): ParametrizedLinear(\n            in_features=3072, out_features=768, bias=False\n            (parametrizations): ModuleDict(\n              (weight): ParametrizationList(\n                (0): LoRAParametrization()\n              )\n            )\n          )\n          (dropout): Dropout(p=0.0, inplace=False)\n        )\n      )\n    )\n    (ln_f): LayerNorm()\n  )\n  (lm_head): ParametrizedLinear(\n    in_features=768, out_features=50304, bias=False\n    (parametrizations): ModuleDict(\n      (weight): ParametrizationList(\n        (0): LoRAParametrization()\n      )\n    )\n  )\n)"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 10.8959, val loss 10.8899\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 0: loss 10.8782, time 54169.51ms, mfu -100.00%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 1: loss 10.8979, time 42671.29ms, mfu -100.00%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 2: loss 10.6887, time 42498.65ms, mfu -100.00%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 3: loss 10.2646, time 42414.63ms, mfu -100.00%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 4: loss 9.9771, time 42286.03ms, mfu -100.00%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 5: loss 9.1891, time 58119.84ms, mfu 0.58%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 6: loss 9.7284, time 42279.96ms, mfu 0.60%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 7: loss 8.4189, time 42266.84ms, mfu 0.62%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 8: loss 8.1264, time 41892.46ms, mfu 0.64%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 9: loss 7.8557, time 41854.66ms, mfu 0.66%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 10: loss 7.6293, time 41912.48ms, mfu 0.67%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 11: loss 7.4288, time 42013.89ms, mfu 0.69%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 12: loss 7.0687, time 52439.34ms, mfu 0.68%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 13: loss 7.1546, time 42471.11ms, mfu 0.69%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 14: loss 6.0358, time 41496.92ms, mfu 0.71%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 15: loss 5.3293, time 41410.92ms, mfu 0.72%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 16: loss 4.9278, time 41468.41ms, mfu 0.73%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 17: loss 4.1874, time 41670.69ms, mfu 0.74%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 18: loss 3.8505, time 46601.32ms, mfu 0.74%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 19: loss 3.7402, time 41818.01ms, mfu 0.74%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 20: loss 3.5946, time 41875.47ms, mfu 0.75%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 21: loss 3.7427, time 41773.42ms, mfu 0.76%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 22: loss 3.8033, time 42687.54ms, mfu 0.76%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 23: loss 3.8768, time 42041.58ms, mfu 0.76%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 24: loss 3.7025, time 42340.58ms, mfu 0.77%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 25: loss 7.9242, time 47155.79ms, mfu 0.76%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 26: loss 4.0177, time 42380.58ms, mfu 0.77%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 27: loss 3.8177, time 42227.86ms, mfu 0.77%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 28: loss 4.2824, time 41913.13ms, mfu 0.77%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 29: loss 3.8785, time 41993.96ms, mfu 0.78%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 30: loss 3.6516, time 42407.80ms, mfu 0.78%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 31: loss 4.2395, time 41965.22ms, mfu 0.78%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 32: loss 3.7936, time 46827.43ms, mfu 0.78%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 33: loss 3.7801, time 42061.43ms, mfu 0.78%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 34: loss 3.7712, time 42302.91ms, mfu 0.78%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 35: loss 3.6603, time 42049.38ms, mfu 0.78%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 36: loss 3.7495, time 42119.83ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 37: loss 3.7202, time 42163.17ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 38: loss 3.6915, time 46841.08ms, mfu 0.78%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 39: loss 3.7839, time 41986.88ms, mfu 0.78%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 40: loss 3.7449, time 41908.29ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 41: loss 3.7259, time 42198.91ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 42: loss 3.7736, time 41772.43ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 43: loss 4.0989, time 41824.06ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 44: loss 3.7298, time 41841.06ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 45: loss 3.7553, time 46202.16ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 46: loss 3.7427, time 41477.59ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 47: loss 3.7737, time 41469.00ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 48: loss 3.7973, time 41507.84ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 49: loss 3.7272, time 41717.31ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 50: loss 3.8261, time 41851.03ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 51: loss 3.6544, time 46688.94ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 52: loss 3.7533, time 41813.80ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 53: loss 4.8939, time 41790.41ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 54: loss 3.7433, time 41785.86ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 55: loss 3.7527, time 41383.92ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 56: loss 3.6975, time 41439.98ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 57: loss 3.6232, time 41519.07ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 58: loss 3.6151, time 46786.89ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 59: loss 4.8710, time 41710.28ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 60: loss 3.7268, time 41883.19ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 61: loss 3.9674, time 41593.60ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 62: loss 3.5366, time 41445.92ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 63: loss 3.8789, time 41481.46ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 64: loss 8.5370, time 46541.50ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 65: loss 3.7930, time 41717.54ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 66: loss 3.4550, time 41802.20ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 67: loss 3.3737, time 41803.15ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 68: loss 3.6298, time 41462.49ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 69: loss 3.3937, time 41497.93ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 70: loss 3.2431, time 41519.77ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 71: loss 3.4284, time 46510.84ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 72: loss 3.3450, time 41803.11ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 73: loss 3.2570, time 41721.98ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 74: loss 3.5315, time 41812.46ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 75: loss 3.5871, time 41934.33ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 76: loss 3.3855, time 41590.98ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 77: loss 3.2325, time 41567.45ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 78: loss 3.4024, time 46236.14ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 79: loss 3.3223, time 41560.92ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 80: loss 3.2411, time 41770.73ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 81: loss 3.1438, time 41755.86ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 82: loss 3.2211, time 41753.36ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 83: loss 3.2755, time 41616.71ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 84: loss 3.5758, time 46613.97ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 85: loss 3.2162, time 41480.45ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 86: loss 3.1233, time 41444.28ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 87: loss 3.2225, time 41445.39ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 88: loss 3.1037, time 41765.89ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 89: loss 3.3279, time 41701.99ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 90: loss 3.0437, time 41857.48ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 91: loss 3.0449, time 46435.34ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 92: loss 3.2331, time 41384.77ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 93: loss 3.1215, time 41360.27ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 94: loss 3.0747, time 41493.24ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 95: loss 3.0648, time 41827.74ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 96: loss 3.1170, time 41705.11ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 97: loss 3.0048, time 46100.21ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 98: loss 3.2308, time 41596.69ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 99: loss 3.1775, time 41420.99ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 100: loss 4.1614, time 41393.16ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 101: loss 2.9590, time 41449.62ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 102: loss 3.0441, time 41690.81ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 103: loss 2.9550, time 46748.62ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 104: loss 2.9863, time 41802.02ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 105: loss 2.9094, time 41741.46ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 106: loss 3.0604, time 41760.84ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 107: loss 2.9825, time 41780.06ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 108: loss 3.1951, time 41667.47ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 109: loss 2.9694, time 41536.03ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 110: loss 2.8923, time 46162.76ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 111: loss 2.8912, time 41583.52ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 112: loss 3.0548, time 41647.06ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 113: loss 2.8516, time 41891.78ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 114: loss 2.8823, time 41849.70ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 115: loss 3.0437, time 41783.25ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 116: loss 3.0443, time 41911.55ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 117: loss 2.9557, time 46511.90ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 118: loss 3.0072, time 41785.79ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 119: loss 3.0659, time 41461.99ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 120: loss 2.9310, time 41597.30ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 121: loss 3.0132, time 41522.46ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 122: loss 3.0914, time 41646.14ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 123: loss 2.9605, time 46626.89ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 124: loss 3.0219, time 41832.99ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 125: loss 3.0744, time 41744.69ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 126: loss 2.8473, time 41771.29ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 127: loss 3.0404, time 41423.51ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 128: loss 3.1207, time 41487.58ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 129: loss 2.7892, time 41565.02ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 130: loss 2.9933, time 46287.55ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 131: loss 2.9641, time 41712.24ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 132: loss 2.9870, time 41725.58ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 133: loss 2.8468, time 41680.38ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 134: loss 2.8052, time 41776.55ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 135: loss 2.8843, time 41677.31ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 136: loss 3.0210, time 46943.83ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 137: loss 3.0481, time 41544.27ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 138: loss 2.9019, time 41424.63ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 139: loss 4.7248, time 41890.99ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 140: loss 2.7973, time 42123.60ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 141: loss 3.1296, time 42278.71ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 142: loss 3.1486, time 41840.97ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 143: loss 2.8921, time 47191.11ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 144: loss 2.9324, time 41803.36ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 145: loss 2.7766, time 41843.36ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 146: loss 2.7918, time 41703.39ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 147: loss 2.7963, time 41711.38ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 148: loss 2.7945, time 41653.16ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 149: loss 2.8557, time 45941.70ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 150: loss 5.6749, time 41453.02ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 151: loss 3.0311, time 41542.11ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 152: loss 3.6314, time 41529.00ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 153: loss 2.7111, time 41655.96ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 154: loss 2.9819, time 41763.95ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 155: loss 2.7831, time 41696.09ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 156: loss 2.8612, time 46389.66ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 157: loss 2.7834, time 41766.73ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 158: loss 2.7955, time 41769.83ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 159: loss 2.7163, time 41509.21ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 160: loss 3.2485, time 41352.21ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 161: loss 2.8300, time 41504.11ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 162: loss 2.7695, time 46459.86ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 163: loss 2.8171, time 41668.23ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 164: loss 2.8327, time 41768.96ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 165: loss 2.7369, time 41964.25ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 166: loss 2.7439, time 42085.49ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 167: loss 2.6882, time 41732.93ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 168: loss 2.6559, time 41640.02ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 169: loss 2.8188, time 52128.27ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 170: loss 2.7663, time 41430.51ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 171: loss 2.7056, time 41446.15ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 172: loss 2.7238, time 41656.54ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 173: loss 2.8125, time 41653.04ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 174: loss 2.8371, time 41762.77ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 175: loss 2.7587, time 46283.22ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 176: loss 2.7601, time 41783.31ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 177: loss 2.7423, time 41803.04ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 178: loss 2.8407, time 41718.32ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 179: loss 2.8742, time 41731.35ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 180: loss 2.7829, time 41510.67ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 181: loss 3.5078, time 41477.68ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 182: loss 2.7462, time 46169.98ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 183: loss 2.6503, time 41728.47ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 184: loss 3.7313, time 41625.51ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 185: loss 2.7360, time 41844.90ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 186: loss 2.7100, time 41752.87ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 187: loss 4.8933, time 41640.75ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 188: loss 2.9776, time 46623.33ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 189: loss 2.6970, time 41660.45ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 190: loss 2.6727, time 41663.91ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 191: loss 2.9421, time 41438.01ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 192: loss 2.7438, time 41420.18ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 193: loss 2.8601, time 41519.17ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 194: loss 2.7353, time 41608.28ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 195: loss 2.7830, time 46239.94ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 196: loss 2.6787, time 41677.18ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 197: loss 2.7099, time 41624.61ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 198: loss 2.7159, time 41816.98ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 199: loss 2.7095, time 41693.55ms, mfu 0.80%\n",
      "step 200: train loss 2.9198, val loss 3.0905\n",
      "saving checkpoint to out\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 200: loss 2.6166, time 64004.06ms, mfu 0.77%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 201: loss 2.7723, time 41362.69ms, mfu 0.78%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 202: loss 2.8053, time 41551.91ms, mfu 0.78%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 203: loss 2.7675, time 41618.99ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 204: loss 2.6120, time 41659.73ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 205: loss 3.1383, time 41760.94ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 206: loss 2.7326, time 41769.42ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 207: loss 2.6236, time 46210.67ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 208: loss 2.8695, time 41742.57ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 209: loss 3.1147, time 41686.28ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 210: loss 2.9614, time 41395.78ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 211: loss 2.7110, time 41613.82ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 212: loss 2.6647, time 41516.95ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 213: loss 2.7592, time 41664.41ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 214: loss 2.7227, time 47145.73ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 215: loss 2.6687, time 41750.84ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 216: loss 2.8600, time 41707.13ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 217: loss 2.6742, time 41761.89ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 218: loss 2.7847, time 41679.34ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 219: loss 2.9488, time 41710.10ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 220: loss 2.7728, time 45742.34ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 221: loss 2.6196, time 41569.32ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 222: loss 3.0459, time 41494.98ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 223: loss 2.6693, time 41660.62ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 224: loss 2.6250, time 41890.86ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 225: loss 2.7427, time 41751.86ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 226: loss 2.6243, time 41691.95ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 227: loss 2.6867, time 46357.82ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 228: loss 2.7260, time 41647.90ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 229: loss 2.6477, time 41756.57ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 230: loss 2.7209, time 41418.82ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 231: loss 7.3260, time 41428.69ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 232: loss 2.6366, time 41607.54ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 233: loss 2.7006, time 46637.28ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 234: loss 2.5579, time 41663.65ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 235: loss 2.7101, time 41842.52ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 236: loss 2.6863, time 41736.05ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 237: loss 2.5792, time 41746.74ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 238: loss 2.9516, time 41755.30ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 239: loss 2.6363, time 41757.38ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 240: loss 2.8302, time 46604.12ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 241: loss 2.7434, time 41434.99ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 242: loss 2.6941, time 41453.94ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 243: loss 4.6637, time 41475.88ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 244: loss 2.6310, time 41770.40ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 245: loss 2.6787, time 41753.34ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 246: loss 2.6048, time 46344.68ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 247: loss 2.6955, time 41658.88ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 248: loss 2.6003, time 41888.01ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 249: loss 2.7070, time 41779.61ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 250: loss 2.7634, time 41704.18ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 251: loss 2.6932, time 41511.12ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 252: loss 2.6010, time 41368.67ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 253: loss 2.9695, time 46185.15ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 254: loss 2.7849, time 41833.18ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 255: loss 2.7723, time 41636.21ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 256: loss 2.9965, time 41713.30ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 257: loss 2.6122, time 41766.71ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 258: loss 2.5929, time 41699.16ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 259: loss 2.8169, time 46452.22ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 260: loss 2.7643, time 41396.67ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 261: loss 2.7721, time 41501.00ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 262: loss 2.6238, time 41690.37ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 263: loss 3.5376, time 41713.73ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 264: loss 2.6903, time 41772.27ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 265: loss 2.7379, time 41703.15ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 266: loss 2.6031, time 46902.31ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 267: loss 3.1770, time 41704.94ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 268: loss 2.5969, time 41737.33ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 269: loss 2.6824, time 41357.65ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 270: loss 4.1492, time 41457.12ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 271: loss 2.5756, time 41422.26ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 272: loss 2.6305, time 42287.93ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 273: loss 2.6516, time 47066.03ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 274: loss 2.6243, time 42198.90ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 275: loss 4.2733, time 42271.52ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 276: loss 3.1121, time 42000.51ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 277: loss 2.6341, time 41955.15ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 278: loss 2.5884, time 41695.43ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 279: loss 2.5426, time 57915.06ms, mfu 0.78%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 280: loss 5.5380, time 41752.82ms, mfu 0.78%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 281: loss 2.8873, time 41851.70ms, mfu 0.78%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 282: loss 2.6136, time 41651.41ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 283: loss 2.7135, time 41613.64ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 284: loss 2.7179, time 41485.28ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 285: loss 2.5428, time 46102.85ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 286: loss 2.6609, time 41545.09ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 287: loss 5.7091, time 41708.21ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 288: loss 2.6556, time 41792.15ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 289: loss 2.6520, time 41720.37ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 290: loss 5.5318, time 41778.05ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 291: loss 2.5866, time 41746.35ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 292: loss 2.7570, time 46400.46ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 293: loss 2.6905, time 41684.41ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 294: loss 2.6311, time 41402.70ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 295: loss 2.6590, time 41426.24ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 296: loss 2.7001, time 41496.70ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 297: loss 2.6623, time 41669.77ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 298: loss 2.8026, time 46361.12ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 299: loss 2.7088, time 41772.67ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 300: loss 2.5785, time 41743.86ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 301: loss 2.7472, time 42111.36ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 302: loss 2.5395, time 41984.43ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 303: loss 2.5294, time 41841.56ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 304: loss 2.6327, time 41936.07ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 305: loss 2.7002, time 46264.61ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 306: loss 2.5620, time 41434.02ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 307: loss 2.6201, time 41467.68ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 308: loss 2.7749, time 41657.67ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 309: loss 5.7193, time 41777.13ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 310: loss 3.6591, time 41815.49ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 311: loss 2.6387, time 41806.67ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 312: loss 2.8772, time 47046.23ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 313: loss 2.5533, time 41852.28ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 314: loss 2.7289, time 41718.39ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 315: loss 2.6760, time 41730.59ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 316: loss 2.6412, time 41472.84ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 317: loss 2.7649, time 41542.10ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 318: loss 2.6215, time 46260.31ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 319: loss 2.6765, time 41744.83ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 320: loss 2.6586, time 41767.26ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 321: loss 2.5472, time 41761.22ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 322: loss 2.6086, time 41730.24ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 323: loss 2.6479, time 41831.06ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 324: loss 2.7842, time 46319.70ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 325: loss 2.5210, time 41644.29ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 326: loss 2.7146, time 41371.62ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 327: loss 2.5917, time 41384.25ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 328: loss 2.5794, time 41555.30ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 329: loss 2.6941, time 41813.11ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 330: loss 2.6273, time 41861.42ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 331: loss 2.5654, time 46415.37ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 332: loss 2.7514, time 41902.64ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 333: loss 2.8242, time 41685.51ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 334: loss 2.6094, time 41741.64ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 335: loss 2.6826, time 41558.01ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 336: loss 2.6962, time 41364.22ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 337: loss 2.6648, time 46936.17ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 338: loss 2.6719, time 41557.43ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 339: loss 2.8362, time 41684.68ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 340: loss 2.5757, time 41737.24ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 341: loss 2.7208, time 41702.76ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 342: loss 2.7361, time 41711.54ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 343: loss 2.6188, time 41713.75ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 344: loss 2.7177, time 46423.34ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 345: loss 2.7090, time 41631.00ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 346: loss 2.5112, time 41379.08ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 347: loss 2.5939, time 41484.32ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 348: loss 2.6825, time 41543.53ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 349: loss 4.0512, time 41812.53ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 350: loss 2.5860, time 41777.51ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 351: loss 2.5576, time 46343.68ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 352: loss 2.5963, time 41674.83ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 353: loss 2.5305, time 41843.96ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 354: loss 2.6911, time 41684.53ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 355: loss 2.5395, time 41707.32ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 356: loss 2.5833, time 41439.96ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 357: loss 2.5207, time 46028.86ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 358: loss 2.5953, time 41508.68ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 359: loss 2.7041, time 41690.45ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 360: loss 2.6814, time 41792.70ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 361: loss 3.4073, time 41841.32ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 362: loss 2.6377, time 41816.85ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 363: loss 2.5525, time 46832.92ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 364: loss 2.5298, time 41782.52ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 365: loss 2.5661, time 41779.15ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 366: loss 2.6545, time 41534.00ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 367: loss 2.6709, time 41729.11ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 368: loss 3.2744, time 41634.03ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 369: loss 2.6795, time 41727.47ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 370: loss 2.6243, time 46976.05ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 371: loss 2.6007, time 41670.73ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 372: loss 2.6967, time 41709.77ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 373: loss 2.5291, time 41837.74ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 374: loss 2.6443, time 41941.41ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 375: loss 3.2906, time 41758.61ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 376: loss 2.6064, time 46531.46ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 377: loss 2.5559, time 41487.75ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 378: loss 2.5590, time 41514.21ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 379: loss 2.6966, time 41641.52ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 380: loss 2.6572, time 41774.35ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 381: loss 2.5907, time 41952.13ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 382: loss 4.7978, time 41744.41ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 383: loss 2.6200, time 46814.35ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 384: loss 2.5720, time 41825.00ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 385: loss 2.5288, time 41945.94ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 386: loss 2.5280, time 41589.06ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 387: loss 2.6107, time 41439.72ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 388: loss 2.5840, time 41562.55ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 389: loss 2.7217, time 41666.43ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 390: loss 4.3894, time 46633.35ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 391: loss 2.4655, time 41825.25ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 392: loss 2.7082, time 41725.11ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 393: loss 4.3664, time 41812.22ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 394: loss 2.5243, time 41802.72ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 395: loss 2.5372, time 41644.19ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 396: loss 2.5339, time 45917.96ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 397: loss 4.6110, time 41468.45ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 398: loss 2.6231, time 41517.35ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 399: loss 2.6338, time 41834.88ms, mfu 0.80%\n",
      "step 400: train loss 2.7165, val loss 3.2087\n",
      "saving checkpoint to out\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 400: loss 2.5664, time 56302.80ms, mfu 0.78%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 401: loss 3.3342, time 41768.80ms, mfu 0.78%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 402: loss 2.5621, time 49448.26ms, mfu 0.77%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 403: loss 2.4656, time 41953.84ms, mfu 0.78%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 404: loss 2.7332, time 41865.19ms, mfu 0.78%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 405: loss 2.6597, time 41685.93ms, mfu 0.78%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 406: loss 2.5456, time 41957.01ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 407: loss 2.6447, time 41987.57ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 408: loss 3.0748, time 43374.27ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 409: loss 2.6527, time 53060.13ms, mfu 0.77%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 410: loss 2.6008, time 41794.05ms, mfu 0.78%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 411: loss 2.5733, time 41767.61ms, mfu 0.78%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 412: loss 2.4959, time 41877.29ms, mfu 0.78%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 413: loss 2.6319, time 41757.76ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 414: loss 4.3479, time 41864.44ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 415: loss 3.5192, time 45881.96ms, mfu 0.78%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 416: loss 3.6344, time 41410.53ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 417: loss 2.6944, time 41442.22ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 418: loss 2.6676, time 41807.51ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 419: loss 2.6805, time 41745.62ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 420: loss 2.5703, time 41987.86ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 421: loss 2.5930, time 41993.57ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 422: loss 2.6705, time 46395.47ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 423: loss 5.7879, time 41675.38ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 424: loss 2.5317, time 41607.63ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 425: loss 2.7155, time 41358.43ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 426: loss 2.8146, time 41435.56ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 427: loss 2.5768, time 41567.33ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 428: loss 2.5901, time 46919.09ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 429: loss 2.5832, time 41794.96ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 430: loss 2.6879, time 41755.29ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 431: loss 2.6503, time 41822.68ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 432: loss 2.5685, time 41847.64ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 433: loss 2.6127, time 41679.64ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 434: loss 2.6072, time 41506.68ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 435: loss 3.4440, time 45557.33ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 436: loss 2.5307, time 41452.34ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 437: loss 2.4968, time 41636.40ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 438: loss 2.5850, time 41650.49ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 439: loss 2.5885, time 41763.01ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 440: loss 2.6128, time 41639.62ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 441: loss 2.6790, time 46568.91ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 442: loss 2.4871, time 41668.89ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 443: loss 2.9736, time 41841.92ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 444: loss 2.5852, time 41396.30ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 445: loss 2.5757, time 41449.06ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 446: loss 2.5940, time 41456.59ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 447: loss 2.5696, time 41665.05ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 448: loss 2.5342, time 46045.45ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 449: loss 2.5026, time 41746.22ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 450: loss 2.4916, time 41758.32ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 451: loss 2.5604, time 41722.00ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 452: loss 2.6897, time 41801.12ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 453: loss 2.7247, time 41637.62ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 454: loss 2.5347, time 45571.74ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 455: loss 2.5645, time 41414.26ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 456: loss 2.6324, time 41716.26ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 457: loss 2.6534, time 41806.79ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 458: loss 2.6309, time 41777.48ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 459: loss 2.6072, time 41667.13ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 460: loss 2.6714, time 41709.12ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 461: loss 2.5341, time 45993.85ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 462: loss 3.5231, time 41943.84ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 463: loss 2.5684, time 41926.10ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 464: loss 2.5015, time 41982.58ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 465: loss 2.5266, time 41922.67ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 466: loss 2.5329, time 41649.49ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 467: loss 2.4611, time 45873.12ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 468: loss 2.5814, time 41731.80ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 469: loss 2.5532, time 41743.79ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 470: loss 2.5670, time 41764.74ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 471: loss 2.5586, time 41602.29ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 472: loss 2.5826, time 41822.67ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 473: loss 2.5473, time 41611.00ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 474: loss 2.6151, time 46120.36ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 475: loss 2.6651, time 41627.77ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 476: loss 2.5463, time 41670.08ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 477: loss 3.1396, time 41618.67ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 478: loss 7.0128, time 41779.06ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 479: loss 2.5045, time 41602.48ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 480: loss 2.7274, time 41672.34ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 481: loss 2.6335, time 46280.22ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 482: loss 3.8051, time 41656.22ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 483: loss 2.7018, time 41829.93ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 484: loss 2.4903, time 41677.83ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 485: loss 2.5374, time 41654.38ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 486: loss 2.5922, time 41647.49ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 487: loss 2.4909, time 46886.26ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 488: loss 2.6043, time 41745.35ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 489: loss 2.5649, time 41699.47ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 490: loss 2.6785, time 41722.39ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 491: loss 2.4115, time 41655.45ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 492: loss 2.5577, time 41579.25ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 493: loss 2.6238, time 41790.84ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 494: loss 2.5565, time 45887.84ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 495: loss 2.5950, time 41673.16ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 496: loss 2.4072, time 41744.08ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 497: loss 2.6562, time 41684.53ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 498: loss 2.6206, time 41764.38ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 499: loss 2.5101, time 41780.38ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 500: loss 2.6145, time 45838.60ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 501: loss 2.4864, time 41719.30ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 502: loss 2.6875, time 41588.75ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 503: loss 2.5525, time 41899.24ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 504: loss 2.5958, time 41642.61ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 505: loss 2.6254, time 41773.69ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 506: loss 2.5854, time 41680.60ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 507: loss 2.5675, time 46069.52ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 508: loss 2.5720, time 41796.96ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 509: loss 2.5136, time 43368.41ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 510: loss 2.5599, time 43480.50ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 511: loss 2.4923, time 43083.46ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 512: loss 2.5831, time 43076.28ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 513: loss 2.4915, time 59264.89ms, mfu 0.77%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 514: loss 2.9771, time 41992.23ms, mfu 0.77%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 515: loss 3.0724, time 41685.51ms, mfu 0.78%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 516: loss 2.5856, time 41674.78ms, mfu 0.78%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 517: loss 3.2910, time 41781.91ms, mfu 0.78%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 518: loss 2.5364, time 41682.77ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 519: loss 6.8934, time 54585.02ms, mfu 0.77%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 520: loss 2.5573, time 41800.99ms, mfu 0.77%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 521: loss 2.4676, time 41759.32ms, mfu 0.78%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 522: loss 2.5280, time 41875.46ms, mfu 0.78%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 523: loss 2.5060, time 42228.55ms, mfu 0.78%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 524: loss 2.4524, time 42157.24ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 525: loss 4.4574, time 41707.21ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 526: loss 2.5599, time 46678.26ms, mfu 0.78%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 527: loss 2.5732, time 41871.52ms, mfu 0.78%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 528: loss 2.5535, time 41942.59ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 529: loss 2.5271, time 41753.09ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 530: loss 2.4670, time 41595.72ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 531: loss 2.4773, time 41584.42ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 532: loss 2.7022, time 41535.67ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 533: loss 3.2496, time 45527.39ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 534: loss 2.5928, time 41607.19ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 535: loss 2.4946, time 41519.50ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 536: loss 2.5195, time 41530.73ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 537: loss 2.4676, time 41593.34ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 538: loss 2.4674, time 41692.34ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 539: loss 2.4414, time 46125.01ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 540: loss 2.4455, time 41481.12ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 541: loss 2.4339, time 41559.87ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 542: loss 2.4095, time 41485.12ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 543: loss 2.5635, time 41456.30ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 544: loss 2.4863, time 41482.16ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 545: loss 2.4787, time 41414.96ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 546: loss 2.5095, time 45687.33ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 547: loss 4.1103, time 41556.24ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 548: loss 2.5086, time 41466.42ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 549: loss 2.7510, time 41455.30ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 550: loss 2.5645, time 41476.16ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 551: loss 2.5806, time 41514.35ms, mfu 0.81%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 552: loss 2.7728, time 41793.72ms, mfu 0.81%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 553: loss 2.4992, time 46186.84ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 554: loss 2.6297, time 41589.32ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 555: loss 2.7783, time 41576.62ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 556: loss 2.4752, time 41581.96ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 557: loss 2.5556, time 41573.16ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 558: loss 2.9745, time 41490.69ms, mfu 0.81%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 559: loss 2.5939, time 46416.80ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 560: loss 2.4848, time 41479.28ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 561: loss 2.5330, time 41499.61ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 562: loss 3.0292, time 41673.27ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 563: loss 2.4388, time 41478.79ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 564: loss 2.5243, time 41619.44ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 565: loss 2.7168, time 41620.08ms, mfu 0.81%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 566: loss 2.4674, time 46071.65ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 567: loss 2.6025, time 41540.76ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 568: loss 2.5104, time 41541.35ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 569: loss 2.5056, time 41432.82ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 570: loss 2.5847, time 41458.28ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 571: loss 2.5529, time 41630.68ms, mfu 0.81%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 572: loss 2.5137, time 51485.89ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 573: loss 3.0845, time 41690.85ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 574: loss 2.4947, time 41633.72ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 575: loss 2.8121, time 41484.96ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 576: loss 2.4188, time 41601.30ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 577: loss 2.5465, time 41532.52ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 578: loss 2.4900, time 41608.63ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 579: loss 2.6918, time 46396.73ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 580: loss 5.3754, time 41737.55ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 581: loss 2.4956, time 41423.09ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 582: loss 2.6279, time 41629.65ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 583: loss 2.5908, time 41600.43ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 584: loss 2.6306, time 41651.73ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 585: loss 2.4910, time 46078.59ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 586: loss 2.6016, time 41640.45ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 587: loss 3.3555, time 41619.62ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 588: loss 2.4880, time 41709.45ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 589: loss 2.5601, time 41595.26ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 590: loss 2.6059, time 41702.79ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 591: loss 2.5392, time 41602.20ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 592: loss 2.7691, time 46111.99ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 593: loss 2.4764, time 41758.08ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 594: loss 2.5679, time 41916.11ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 595: loss 2.6394, time 43018.02ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 596: loss 2.5741, time 41839.55ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 597: loss 2.6354, time 42140.43ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 598: loss 2.7089, time 46380.64ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 599: loss 2.5754, time 41841.07ms, mfu 0.79%\n",
      "step 600: train loss 2.6887, val loss 2.7998\n",
      "saving checkpoint to out\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 600: loss 2.4684, time 56154.20ms, mfu 0.78%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 601: loss 2.5378, time 41907.49ms, mfu 0.78%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 602: loss 2.7363, time 41837.20ms, mfu 0.78%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 603: loss 3.3608, time 41633.70ms, mfu 0.78%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 604: loss 2.6489, time 48080.15ms, mfu 0.78%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 605: loss 2.5735, time 41676.01ms, mfu 0.78%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 606: loss 2.6693, time 41753.28ms, mfu 0.78%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 607: loss 2.9207, time 41845.74ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 608: loss 3.2923, time 41678.09ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 609: loss 2.7276, time 41772.68ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 610: loss 2.8628, time 42220.51ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 611: loss 2.4772, time 46792.53ms, mfu 0.78%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 612: loss 2.8560, time 42653.52ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 613: loss 2.4315, time 41730.89ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 614: loss 2.4935, time 41833.82ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 615: loss 2.5370, time 41947.71ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 616: loss 2.6890, time 42950.40ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 617: loss 2.5047, time 42203.11ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 618: loss 2.5969, time 46834.42ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 619: loss 2.7188, time 41701.64ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 620: loss 4.8889, time 41690.69ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 621: loss 2.6717, time 41702.52ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 622: loss 2.5235, time 41853.87ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 623: loss 2.5089, time 41871.15ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 624: loss 3.4840, time 45850.30ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 625: loss 2.5838, time 41704.93ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 626: loss 2.7104, time 41729.95ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 627: loss 2.6387, time 41752.00ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 628: loss 2.6065, time 41791.51ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 629: loss 3.1847, time 41670.33ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 630: loss 2.5792, time 41855.32ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 631: loss 2.5382, time 46053.56ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 632: loss 4.2730, time 42255.72ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 633: loss 2.5895, time 42609.35ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 634: loss 2.7354, time 43107.98ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 635: loss 2.6920, time 43082.06ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 636: loss 2.5469, time 42623.95ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 637: loss 2.5399, time 42772.93ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 638: loss 2.4877, time 52300.42ms, mfu 0.78%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 639: loss 2.5485, time 42651.48ms, mfu 0.78%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 640: loss 2.4872, time 41779.29ms, mfu 0.78%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 641: loss 2.5778, time 41921.08ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 642: loss 2.5817, time 41898.28ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 643: loss 2.6049, time 41791.82ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 644: loss 3.2449, time 53759.85ms, mfu 0.77%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 645: loss 2.4562, time 44756.28ms, mfu 0.77%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 646: loss 2.5719, time 43960.79ms, mfu 0.77%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 647: loss 2.6468, time 43835.74ms, mfu 0.77%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 648: loss 2.4934, time 43677.57ms, mfu 0.77%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 649: loss 2.6420, time 43453.23ms, mfu 0.77%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 650: loss 2.5667, time 43104.67ms, mfu 0.77%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 651: loss 6.0321, time 53569.02ms, mfu 0.76%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 652: loss 2.9554, time 41917.45ms, mfu 0.76%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 653: loss 2.6277, time 41878.75ms, mfu 0.77%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 654: loss 2.5035, time 41748.20ms, mfu 0.77%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 655: loss 2.6941, time 41694.43ms, mfu 0.78%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 656: loss 2.4334, time 41763.78ms, mfu 0.78%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 657: loss 2.4883, time 45975.65ms, mfu 0.78%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 658: loss 2.5701, time 41710.59ms, mfu 0.78%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 659: loss 2.6388, time 41797.33ms, mfu 0.78%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 660: loss 2.5771, time 41728.73ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 661: loss 2.5001, time 41686.14ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 662: loss 2.6830, time 41803.50ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 663: loss 2.5757, time 42059.55ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 664: loss 2.4819, time 46178.33ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 665: loss 2.5403, time 41720.98ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 666: loss 3.8611, time 41627.73ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 667: loss 2.5048, time 41716.71ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 668: loss 2.4976, time 41772.34ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 669: loss 2.5729, time 41887.77ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 670: loss 2.5176, time 45854.52ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 671: loss 3.6111, time 41702.77ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 672: loss 2.4360, time 41743.06ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 673: loss 2.4667, time 41864.69ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 674: loss 3.4650, time 41771.29ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 675: loss 2.5734, time 41644.37ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 676: loss 2.4281, time 41728.55ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 677: loss 2.4549, time 46415.50ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 678: loss 2.5145, time 41684.09ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 679: loss 2.5822, time 41948.57ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 680: loss 2.5514, time 41994.23ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 681: loss 2.6224, time 41962.16ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 682: loss 2.6385, time 41777.82ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 683: loss 2.4751, time 41738.18ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 684: loss 2.4640, time 45800.22ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 685: loss 2.4742, time 41732.14ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 686: loss 2.4549, time 41771.70ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 687: loss 4.2783, time 41943.02ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 688: loss 2.6484, time 41675.64ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 689: loss 2.5449, time 41795.17ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 690: loss 2.6161, time 45834.01ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 691: loss 2.5790, time 41836.92ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 692: loss 2.5114, time 41832.66ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 693: loss 2.6415, time 41727.07ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 694: loss 2.5975, time 41869.28ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 695: loss 2.4968, time 42064.54ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 696: loss 2.4136, time 41785.29ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 697: loss 2.4906, time 46402.20ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 698: loss 2.4428, time 42811.04ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 699: loss 2.5235, time 43373.16ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 700: loss 2.5981, time 43205.82ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 701: loss 2.5080, time 43209.92ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 702: loss 2.5569, time 43051.70ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 703: loss 2.5636, time 57642.52ms, mfu 0.77%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 704: loss 2.5337, time 42255.90ms, mfu 0.77%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 705: loss 2.5030, time 41920.53ms, mfu 0.78%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 706: loss 2.5063, time 41942.16ms, mfu 0.78%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 707: loss 2.5300, time 41720.71ms, mfu 0.78%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 708: loss 2.6481, time 41710.64ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 709: loss 2.5228, time 41797.40ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 710: loss 2.5925, time 45834.75ms, mfu 0.78%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 711: loss 2.5846, time 41669.51ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 712: loss 2.6228, time 41644.81ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 713: loss 2.5234, time 41646.30ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 714: loss 2.5458, time 41704.44ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 715: loss 2.4578, time 41791.65ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 716: loss 2.5484, time 46293.16ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 717: loss 2.8431, time 41721.28ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 718: loss 2.5443, time 41696.54ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 719: loss 2.5480, time 41721.51ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 720: loss 2.6118, time 41669.22ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 721: loss 2.5734, time 41715.83ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 722: loss 2.4602, time 41909.47ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 723: loss 2.4552, time 45830.50ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 724: loss 2.5150, time 41681.86ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 725: loss 2.8241, time 41710.29ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 726: loss 2.6028, time 41743.27ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 727: loss 5.7775, time 41861.35ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 728: loss 2.4323, time 41661.06ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 729: loss 2.5626, time 46469.47ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 730: loss 2.4754, time 42042.65ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 731: loss 2.8512, time 41833.33ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 732: loss 2.5183, time 41782.12ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 733: loss 5.1307, time 41691.75ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 734: loss 2.4194, time 41739.08ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 735: loss 2.5180, time 41702.17ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 736: loss 2.5377, time 46018.62ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 737: loss 3.2289, time 42025.38ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 738: loss 2.4599, time 41824.33ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 739: loss 2.5679, time 41954.27ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 740: loss 2.6415, time 41874.36ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 741: loss 4.2767, time 41833.11ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 742: loss 2.6136, time 46682.41ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 743: loss 2.5246, time 42118.73ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 744: loss 2.4890, time 41765.20ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 745: loss 2.5493, time 41835.98ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 746: loss 2.4904, time 41630.73ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 747: loss 2.5109, time 41763.11ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 748: loss 2.4743, time 41657.69ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 749: loss 2.4800, time 46103.55ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 750: loss 2.5923, time 41710.53ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 751: loss 2.5436, time 41783.75ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 752: loss 2.6278, time 41689.10ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 753: loss 2.5887, time 41701.03ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 754: loss 2.5729, time 41745.51ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 755: loss 2.4930, time 41925.54ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 756: loss 2.5922, time 46481.55ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 757: loss 4.3514, time 41889.17ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 758: loss 2.6197, time 42134.44ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 759: loss 2.4609, time 41865.29ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 760: loss 2.5070, time 42172.53ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 761: loss 2.6439, time 42121.50ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 762: loss 2.5689, time 52700.65ms, mfu 0.78%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 763: loss 2.5689, time 42136.47ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 764: loss 2.3968, time 42285.08ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 765: loss 2.5988, time 41810.81ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 766: loss 2.4494, time 41735.60ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 767: loss 2.4626, time 41744.14ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 768: loss 2.4820, time 41698.90ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 769: loss 2.5442, time 45875.71ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 770: loss 2.5774, time 41685.87ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 771: loss 2.5805, time 41661.50ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 772: loss 2.5146, time 41730.68ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 773: loss 3.0118, time 41734.06ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 774: loss 2.5833, time 41654.77ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 775: loss 2.5348, time 46329.43ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 776: loss 2.5597, time 41716.56ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 777: loss 2.5350, time 41978.74ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 778: loss 2.7083, time 41900.79ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 779: loss 2.5279, time 41873.73ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 780: loss 6.5697, time 41626.73ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 781: loss 2.5130, time 41742.70ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 782: loss 2.5918, time 45921.98ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 783: loss 2.4533, time 41971.72ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 784: loss 5.2569, time 41837.36ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 785: loss 2.4877, time 41777.71ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 786: loss 2.5283, time 42038.79ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 787: loss 2.6404, time 42073.71ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 788: loss 2.5129, time 46162.33ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 789: loss 3.3438, time 41959.57ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 790: loss 2.4924, time 41825.97ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 791: loss 2.5371, time 41809.40ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 792: loss 2.5979, time 41768.64ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 793: loss 3.4326, time 41964.20ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 794: loss 2.4528, time 41886.50ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 795: loss 2.5611, time 46054.40ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 796: loss 2.5124, time 41777.62ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 797: loss 2.7019, time 41726.06ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 798: loss 2.4860, time 42765.95ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 799: loss 2.5652, time 43057.77ms, mfu 0.80%\n",
      "step 800: train loss 2.7877, val loss 2.5885\n",
      "saving checkpoint to out\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 800: loss 2.6106, time 57888.53ms, mfu 0.77%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 801: loss 2.4876, time 59066.23ms, mfu 0.75%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 802: loss 2.6585, time 43139.98ms, mfu 0.76%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 803: loss 2.8793, time 43219.49ms, mfu 0.76%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 804: loss 2.4725, time 42456.96ms, mfu 0.76%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 805: loss 2.6710, time 41903.18ms, mfu 0.77%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 806: loss 2.6047, time 41722.92ms, mfu 0.77%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 807: loss 2.5311, time 53415.60ms, mfu 0.76%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 808: loss 2.6744, time 41756.58ms, mfu 0.76%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 809: loss 2.7552, time 41834.84ms, mfu 0.77%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 810: loss 2.5612, time 41805.94ms, mfu 0.77%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 811: loss 2.9632, time 41943.02ms, mfu 0.78%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 812: loss 2.5441, time 41636.11ms, mfu 0.78%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 813: loss 2.3726, time 41747.22ms, mfu 0.78%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 814: loss 2.5631, time 46251.27ms, mfu 0.78%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 815: loss 2.4656, time 41957.69ms, mfu 0.78%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 816: loss 2.4933, time 42362.47ms, mfu 0.78%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 817: loss 2.4533, time 41645.07ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 818: loss 4.9317, time 41961.25ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 819: loss 2.4193, time 41674.65ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 820: loss 3.1905, time 46491.49ms, mfu 0.78%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 821: loss 2.5631, time 41977.49ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 822: loss 2.5318, time 42339.62ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 823: loss 2.5188, time 41593.09ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 824: loss 2.8033, time 42281.04ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 825: loss 2.4512, time 41849.94ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 826: loss 2.5748, time 41692.64ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 827: loss 2.5697, time 46924.47ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 828: loss 2.4792, time 42134.29ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 829: loss 2.4290, time 42290.58ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 830: loss 2.5351, time 41917.70ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 831: loss 2.4705, time 41896.64ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 832: loss 2.5041, time 42069.83ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 833: loss 2.6597, time 46117.32ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 834: loss 2.6538, time 41784.06ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 835: loss 2.5649, time 41989.19ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 836: loss 2.6209, time 41773.70ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 837: loss 2.4092, time 41768.31ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 838: loss 2.5352, time 41790.40ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 839: loss 2.4674, time 41827.56ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 840: loss 2.9254, time 45748.91ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 841: loss 2.5907, time 41924.70ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 842: loss 2.6030, time 41647.69ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 843: loss 2.5678, time 42137.12ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 844: loss 2.5713, time 42153.60ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 845: loss 2.5319, time 41871.03ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 846: loss 6.3543, time 46152.45ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 847: loss 2.3454, time 41986.18ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 848: loss 2.6426, time 41978.35ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 849: loss 2.5372, time 41779.84ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 850: loss 2.4419, time 41715.79ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 851: loss 3.4876, time 41917.47ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 852: loss 2.7172, time 42185.41ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 853: loss 2.5970, time 51722.06ms, mfu 0.78%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 854: loss 2.8151, time 42272.95ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 855: loss 2.4457, time 42641.90ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 856: loss 2.6971, time 42278.41ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 857: loss 2.8057, time 42216.10ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 858: loss 2.4827, time 42128.80ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 859: loss 2.5799, time 42024.94ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 860: loss 2.6018, time 45972.61ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 861: loss 2.4926, time 42398.07ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 862: loss 2.4763, time 41985.63ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 863: loss 2.5300, time 42143.81ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 864: loss 2.4385, time 42017.11ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 865: loss 2.6093, time 41968.13ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 866: loss 2.4608, time 45874.45ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 867: loss 2.4509, time 42086.67ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 868: loss 2.4466, time 41979.63ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 869: loss 2.4336, time 41729.60ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 870: loss 2.5305, time 41624.72ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 871: loss 2.4841, time 41937.57ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 872: loss 2.4955, time 46188.46ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 873: loss 2.4151, time 41763.22ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 874: loss 2.6095, time 41589.31ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 875: loss 2.5161, time 41855.24ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 876: loss 2.5632, time 41814.39ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 877: loss 2.4775, time 41729.99ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 878: loss 2.5036, time 41741.36ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 879: loss 2.5229, time 45740.61ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 880: loss 3.4884, time 41543.22ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 881: loss 2.4618, time 41817.24ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 882: loss 2.5794, time 41674.66ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 883: loss 2.5351, time 42062.69ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 884: loss 2.5440, time 41759.43ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 885: loss 6.3662, time 45938.40ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 886: loss 2.4656, time 42068.54ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 887: loss 2.5470, time 42646.25ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 888: loss 2.4970, time 41834.08ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 889: loss 2.6115, time 41726.87ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 890: loss 6.0878, time 42101.51ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 891: loss 2.4847, time 42348.36ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 892: loss 2.6316, time 46081.68ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 893: loss 2.5069, time 41895.94ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 894: loss 2.5450, time 41929.38ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 895: loss 2.7033, time 42106.96ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 896: loss 2.5142, time 41647.59ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 897: loss 4.3529, time 41637.86ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 898: loss 2.5099, time 46420.41ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 899: loss 2.4868, time 42183.75ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 900: loss 6.0502, time 42926.99ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 901: loss 2.5428, time 41800.92ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 902: loss 2.6080, time 42002.53ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 903: loss 2.5876, time 42428.24ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 904: loss 2.5134, time 41924.04ms, mfu 0.80%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 905: loss 2.4279, time 46119.59ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 906: loss 2.4058, time 41847.25ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 907: loss 2.5633, time 43215.61ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 908: loss 2.4194, time 43252.96ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 909: loss 2.9633, time 43321.75ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 910: loss 2.5128, time 42920.95ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 911: loss 2.6054, time 59147.43ms, mfu 0.77%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 912: loss 2.4679, time 42781.58ms, mfu 0.77%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 913: loss 2.5868, time 42724.21ms, mfu 0.77%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 914: loss 2.6487, time 41871.16ms, mfu 0.78%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 915: loss 2.5465, time 41879.93ms, mfu 0.78%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 916: loss 2.6165, time 42166.69ms, mfu 0.78%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 917: loss 2.4958, time 41648.86ms, mfu 0.78%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 918: loss 2.6891, time 54840.21ms, mfu 0.77%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 919: loss 2.7315, time 41774.13ms, mfu 0.77%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 920: loss 2.6559, time 41698.88ms, mfu 0.78%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 921: loss 2.6062, time 41957.32ms, mfu 0.78%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 922: loss 2.6495, time 41832.28ms, mfu 0.78%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 923: loss 2.5136, time 41937.08ms, mfu 0.78%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 924: loss 2.5813, time 46197.61ms, mfu 0.78%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 925: loss 3.0903, time 42220.81ms, mfu 0.78%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 926: loss 2.5177, time 41741.47ms, mfu 0.78%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 927: loss 2.4989, time 42192.06ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 928: loss 2.6706, time 42559.40ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 929: loss 2.5237, time 42170.22ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 930: loss 2.4193, time 46602.11ms, mfu 0.78%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 931: loss 2.6638, time 42121.56ms, mfu 0.78%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 932: loss 2.6456, time 42769.98ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 933: loss 2.7548, time 42397.81ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 934: loss 2.5352, time 42010.88ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 935: loss 2.6304, time 42373.32ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 936: loss 2.5936, time 42275.66ms, mfu 0.79%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 937: loss 2.4498, time 211388.05ms, mfu 0.73%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 938: loss 2.3940, time 1055065.22ms, mfu 0.66%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 939: loss 2.4732, time 43038.34ms, mfu 0.67%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 940: loss 2.6227, time 50905.58ms, mfu 0.67%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 941: loss 2.4765, time 88465.15ms, mfu 0.64%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 942: loss 3.5960, time 140866.66ms, mfu 0.60%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 943: loss 2.5068, time 158797.10ms, mfu 0.56%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 944: loss 2.4094, time 170196.42ms, mfu 0.53%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 945: loss 2.5297, time 180560.18ms, mfu 0.49%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 946: loss 2.5464, time 200428.44ms, mfu 0.46%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 947: loss 2.5183, time 206963.63ms, mfu 0.43%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 948: loss 2.7539, time 483047.69ms, mfu 0.39%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 949: loss 3.6226, time 42097.16ms, mfu 0.44%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 950: loss 3.2307, time 960431.19ms, mfu 0.40%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 951: loss 2.9737, time 42752.69ms, mfu 0.44%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 952: loss 2.8783, time 926548.39ms, mfu 0.40%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 953: loss 2.7899, time 42354.18ms, mfu 0.44%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 954: loss 2.6499, time 50364.29ms, mfu 0.46%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 955: loss 2.7980, time 79417.08ms, mfu 0.46%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 956: loss 2.6630, time 118147.17ms, mfu 0.44%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 957: loss 2.7841, time 135138.04ms, mfu 0.42%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 958: loss 2.6389, time 140382.63ms, mfu 0.40%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 959: loss 2.6895, time 171660.47ms, mfu 0.38%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 960: loss 2.7671, time 171758.26ms, mfu 0.36%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 961: loss 2.7416, time 180338.51ms, mfu 0.35%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 962: loss 3.3830, time 202705.31ms, mfu 0.33%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 963: loss 2.6398, time 208116.84ms, mfu 0.31%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 964: loss 3.6290, time 216616.20ms, mfu 0.30%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 965: loss 3.6182, time 212903.70ms, mfu 0.28%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 966: loss 5.0510, time 210616.36ms, mfu 0.27%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 967: loss 4.3919, time 213720.67ms, mfu 0.26%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 968: loss 4.0235, time 202647.50ms, mfu 0.25%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 969: loss 4.9360, time 205400.61ms, mfu 0.24%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 970: loss 6.2704, time 215883.79ms, mfu 0.23%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 971: loss 10.1423, time 213057.81ms, mfu 0.23%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 972: loss 12.2543, time 206009.65ms, mfu 0.22%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 973: loss 11.2423, time 210502.52ms, mfu 0.21%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 974: loss 8.5789, time 230758.98ms, mfu 0.21%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 975: loss 8.8717, time 200425.56ms, mfu 0.20%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 976: loss 6.9978, time 210055.53ms, mfu 0.20%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 977: loss 6.6429, time 196541.96ms, mfu 0.20%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 978: loss 7.0888, time 221990.64ms, mfu 0.19%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 979: loss 5.9675, time 214888.45ms, mfu 0.19%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 980: loss 9.9692, time 216445.04ms, mfu 0.19%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 981: loss 4.8256, time 208935.14ms, mfu 0.18%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 982: loss 4.1769, time 199156.01ms, mfu 0.18%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 983: loss 4.0856, time 224916.27ms, mfu 0.18%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 984: loss 4.2425, time 209615.97ms, mfu 0.18%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 985: loss 4.4712, time 218785.97ms, mfu 0.17%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 986: loss 4.8479, time 228459.34ms, mfu 0.17%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 987: loss 5.7999, time 202564.83ms, mfu 0.17%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 988: loss 7.5533, time 209010.90ms, mfu 0.17%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 989: loss 7.5764, time 213226.06ms, mfu 0.17%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 990: loss 6.4983, time 214146.57ms, mfu 0.17%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 991: loss 6.8479, time 212766.39ms, mfu 0.17%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 992: loss 7.1915, time 206376.72ms, mfu 0.17%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 993: loss 7.9108, time 223632.10ms, mfu 0.17%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 994: loss 8.6973, time 214858.08ms, mfu 0.16%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 995: loss 8.6569, time 207304.48ms, mfu 0.16%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 996: loss 8.4602, time 199805.61ms, mfu 0.16%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 997: loss 10.9133, time 215573.41ms, mfu 0.16%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 998: loss 12.3416, time 203520.33ms, mfu 0.16%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 999: loss 14.8175, time 216057.29ms, mfu 0.16%\n",
      "step 1000: train loss 15.3598, val loss 15.4523\n",
      "saving checkpoint to out\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 1000: loss 14.8547, time 306761.21ms, mfu 0.16%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 1001: loss 13.8478, time 204582.21ms, mfu 0.16%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 1002: loss 14.1738, time 214562.17ms, mfu 0.16%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 1003: loss 15.6245, time 209814.89ms, mfu 0.16%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 1004: loss 18.8426, time 209541.73ms, mfu 0.16%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 1005: loss 20.8083, time 216360.25ms, mfu 0.16%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 1006: loss 23.5430, time 210363.95ms, mfu 0.16%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 1007: loss 23.6434, time 210584.57ms, mfu 0.16%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 1008: loss 22.4080, time 216625.54ms, mfu 0.16%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 1009: loss 18.9188, time 226320.26ms, mfu 0.16%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 1010: loss 14.0360, time 213892.37ms, mfu 0.16%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 1011: loss 13.3982, time 211517.82ms, mfu 0.16%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 1012: loss 14.3571, time 217567.47ms, mfu 0.16%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 1013: loss 15.0139, time 195598.61ms, mfu 0.16%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 1014: loss 16.5825, time 219229.79ms, mfu 0.16%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 1015: loss 17.7318, time 210709.26ms, mfu 0.16%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 1016: loss 19.4419, time 209887.31ms, mfu 0.16%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 1017: loss 20.1280, time 212505.86ms, mfu 0.16%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 1018: loss 22.2880, time 229930.21ms, mfu 0.16%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 1019: loss 21.2521, time 217366.03ms, mfu 0.16%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 1020: loss 20.7247, time 220849.51ms, mfu 0.16%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 1021: loss 19.9329, time 209810.98ms, mfu 0.16%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 1022: loss 19.7214, time 224163.49ms, mfu 0.16%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 1023: loss 20.6871, time 218298.12ms, mfu 0.16%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 1024: loss 20.5056, time 210765.80ms, mfu 0.16%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 1025: loss 21.5280, time 208828.25ms, mfu 0.16%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 1026: loss 20.3305, time 219552.23ms, mfu 0.16%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 1027: loss 22.7274, time 222719.65ms, mfu 0.16%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 1028: loss 18.6825, time 235451.31ms, mfu 0.16%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 1029: loss 18.3525, time 202280.90ms, mfu 0.16%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 1030: loss 19.0573, time 214028.52ms, mfu 0.16%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 1031: loss 19.0416, time 204152.18ms, mfu 0.16%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 1032: loss 18.8870, time 225417.05ms, mfu 0.16%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 1033: loss 18.3775, time 233417.27ms, mfu 0.16%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 1034: loss 18.4079, time 236475.83ms, mfu 0.15%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 1035: loss 19.4834, time 214080.28ms, mfu 0.15%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 1036: loss 19.1736, time 223979.32ms, mfu 0.15%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 1037: loss 18.5770, time 55253.84ms, mfu 0.20%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 1038: loss 19.0785, time 44369.35ms, mfu 0.26%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 1039: loss 16.8930, time 44356.67ms, mfu 0.31%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 1040: loss 14.9063, time 59409.17ms, mfu 0.33%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 1041: loss 12.9144, time 44911.31ms, mfu 0.38%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 1042: loss 12.5598, time 44943.62ms, mfu 0.41%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 1043: loss 10.5735, time 44093.94ms, mfu 0.45%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 1044: loss 9.6070, time 44070.43ms, mfu 0.48%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 1045: loss 9.5645, time 43656.96ms, mfu 0.51%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 1046: loss 8.2970, time 44099.59ms, mfu 0.54%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 1047: loss 8.8673, time 59936.21ms, mfu 0.54%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 1048: loss 7.5319, time 44799.49ms, mfu 0.56%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 1049: loss 7.0511, time 44107.86ms, mfu 0.58%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 1050: loss 6.4365, time 43737.56ms, mfu 0.60%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 1051: loss 6.5374, time 43874.03ms, mfu 0.62%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 1052: loss 6.4220, time 44412.40ms, mfu 0.63%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 1053: loss 6.0168, time 43699.68ms, mfu 0.65%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 1054: loss 5.2889, time 48559.83ms, mfu 0.65%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 1055: loss 4.7518, time 43830.72ms, mfu 0.66%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 1056: loss 5.3342, time 44448.52ms, mfu 0.67%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 1057: loss 4.5527, time 44793.09ms, mfu 0.68%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 1058: loss 4.5042, time 44309.15ms, mfu 0.69%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 1059: loss 4.6027, time 44117.70ms, mfu 0.70%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 1060: loss 4.2387, time 49037.41ms, mfu 0.70%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 1061: loss 4.1879, time 44777.06ms, mfu 0.70%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 1062: loss 4.0271, time 43735.82ms, mfu 0.71%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 1063: loss 4.5755, time 44055.25ms, mfu 0.72%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 1064: loss 3.9860, time 43181.06ms, mfu 0.72%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 1065: loss 3.9318, time 43704.11ms, mfu 0.73%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 1066: loss 4.1385, time 43905.59ms, mfu 0.73%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 1067: loss 4.0113, time 54357.65ms, mfu 0.72%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 1068: loss 3.7761, time 45875.14ms, mfu 0.72%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 1069: loss 3.7315, time 44932.29ms, mfu 0.73%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 1070: loss 8.4586, time 44407.48ms, mfu 0.73%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 1071: loss 3.5411, time 44140.43ms, mfu 0.73%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 1072: loss 3.4427, time 44089.86ms, mfu 0.74%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 1073: loss 3.7013, time 54340.78ms, mfu 0.73%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 1074: loss 3.8154, time 44280.10ms, mfu 0.73%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 1075: loss 4.3585, time 43799.11ms, mfu 0.73%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 1076: loss 3.7790, time 43667.55ms, mfu 0.74%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 1077: loss 6.4409, time 44263.95ms, mfu 0.74%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 1078: loss 3.4888, time 44400.77ms, mfu 0.74%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 1079: loss 3.3998, time 44156.52ms, mfu 0.75%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 1080: loss 3.4434, time 53546.45ms, mfu 0.73%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 1081: loss 3.4912, time 43080.47ms, mfu 0.74%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 1082: loss 3.4852, time 42894.92ms, mfu 0.74%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 1083: loss 3.3802, time 42769.97ms, mfu 0.75%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 1084: loss 3.6465, time 43163.87ms, mfu 0.75%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 1085: loss 3.3009, time 44095.08ms, mfu 0.75%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 1086: loss 3.3475, time 48986.88ms, mfu 0.75%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 1087: loss 3.2736, time 44038.83ms, mfu 0.75%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 1088: loss 4.8089, time 43961.47ms, mfu 0.75%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 1089: loss 3.2021, time 43815.91ms, mfu 0.75%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 1090: loss 3.2529, time 43973.95ms, mfu 0.76%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 1091: loss 3.1518, time 44078.44ms, mfu 0.76%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 1092: loss 3.1715, time 44343.95ms, mfu 0.76%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 1093: loss 3.1216, time 48783.95ms, mfu 0.75%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 1094: loss 3.1622, time 44211.75ms, mfu 0.75%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 1095: loss 3.0843, time 44315.29ms, mfu 0.75%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 1096: loss 3.3194, time 44292.62ms, mfu 0.75%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 1097: loss 3.0504, time 44335.52ms, mfu 0.76%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 1098: loss 3.1393, time 44250.68ms, mfu 0.76%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 1099: loss 3.4290, time 48149.67ms, mfu 0.75%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 1100: loss 3.0934, time 43848.45ms, mfu 0.75%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 1101: loss 3.2082, time 43960.51ms, mfu 0.75%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 1102: loss 3.1732, time 43940.37ms, mfu 0.76%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 1103: loss 3.0296, time 44484.76ms, mfu 0.76%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 1104: loss 2.9653, time 43633.49ms, mfu 0.76%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 1105: loss 2.9546, time 44575.52ms, mfu 0.76%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 1106: loss 3.0380, time 56084.85ms, mfu 0.74%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 1107: loss 3.1234, time 44921.71ms, mfu 0.74%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 1108: loss 3.0952, time 44460.31ms, mfu 0.75%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 1109: loss 3.0718, time 44718.41ms, mfu 0.75%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 1110: loss 3.0568, time 46511.24ms, mfu 0.75%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 1111: loss 3.0198, time 45162.55ms, mfu 0.75%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 1112: loss 3.0173, time 50470.17ms, mfu 0.74%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 1113: loss 3.0688, time 44328.41ms, mfu 0.74%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 1114: loss 2.9380, time 44602.20ms, mfu 0.74%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 1115: loss 2.9087, time 44380.08ms, mfu 0.74%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 1116: loss 2.9502, time 44494.62ms, mfu 0.75%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 1117: loss 2.9951, time 44057.19ms, mfu 0.75%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 1118: loss 3.1142, time 54712.49ms, mfu 0.74%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 1119: loss 2.9777, time 44470.53ms, mfu 0.74%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 1120: loss 2.9577, time 44992.01ms, mfu 0.74%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 1121: loss 3.0780, time 44100.14ms, mfu 0.74%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 1122: loss 3.0525, time 42838.24ms, mfu 0.75%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 1123: loss 3.0607, time 43328.51ms, mfu 0.75%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 1124: loss 2.9205, time 43674.55ms, mfu 0.75%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 1125: loss 3.3311, time 53589.70ms, mfu 0.74%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 1126: loss 2.7368, time 43622.31ms, mfu 0.74%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 1127: loss 2.9305, time 43036.58ms, mfu 0.75%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 1128: loss 2.8964, time 43389.68ms, mfu 0.75%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 1129: loss 2.9155, time 43134.34ms, mfu 0.76%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 1130: loss 2.7750, time 44039.00ms, mfu 0.76%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 1131: loss 2.9229, time 48422.61ms, mfu 0.75%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 1132: loss 2.8276, time 43512.06ms, mfu 0.75%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 1133: loss 2.7903, time 43151.73ms, mfu 0.76%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 1134: loss 2.9345, time 44131.77ms, mfu 0.76%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 1135: loss 2.8842, time 45654.76ms, mfu 0.76%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 1136: loss 2.9852, time 44500.08ms, mfu 0.76%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 1137: loss 2.8253, time 45255.97ms, mfu 0.76%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 1138: loss 2.9206, time 55116.33ms, mfu 0.74%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 1139: loss 7.7450, time 44377.11ms, mfu 0.74%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 1140: loss 2.9258, time 44164.84ms, mfu 0.75%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 1141: loss 2.7806, time 44391.24ms, mfu 0.75%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 1142: loss 2.7816, time 44290.38ms, mfu 0.75%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 1143: loss 2.7725, time 44839.67ms, mfu 0.75%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 1144: loss 2.7800, time 44335.98ms, mfu 0.75%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 1145: loss 2.8348, time 49373.74ms, mfu 0.74%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 1146: loss 2.8079, time 44439.21ms, mfu 0.75%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 1147: loss 3.8872, time 47554.44ms, mfu 0.74%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 1148: loss 2.7631, time 45927.63ms, mfu 0.74%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 1149: loss 3.4056, time 44212.27ms, mfu 0.74%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 1150: loss 2.9518, time 44471.76ms, mfu 0.75%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 1151: loss 2.8463, time 49227.45ms, mfu 0.74%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 1152: loss 2.8495, time 44655.88ms, mfu 0.74%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 1153: loss 2.7740, time 44149.58ms, mfu 0.74%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 1154: loss 2.7688, time 44346.48ms, mfu 0.75%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 1155: loss 2.8068, time 43925.54ms, mfu 0.75%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 1156: loss 2.7554, time 44294.66ms, mfu 0.75%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 1157: loss 2.7054, time 44160.78ms, mfu 0.75%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 1158: loss 2.7226, time 48400.73ms, mfu 0.75%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 1159: loss 2.7161, time 45296.59ms, mfu 0.75%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 1160: loss 6.0809, time 45092.70ms, mfu 0.75%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 1161: loss 2.6862, time 44555.75ms, mfu 0.75%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 1162: loss 2.6901, time 44101.75ms, mfu 0.75%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 1163: loss 2.8382, time 44565.17ms, mfu 0.75%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 1164: loss 2.8452, time 62852.23ms, mfu 0.73%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 1165: loss 2.9483, time 44337.62ms, mfu 0.73%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 1166: loss 2.7377, time 46332.90ms, mfu 0.73%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 1167: loss 2.8861, time 43906.47ms, mfu 0.74%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 1168: loss 2.7718, time 43972.61ms, mfu 0.74%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 1169: loss 2.7324, time 43607.96ms, mfu 0.74%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 1170: loss 2.6612, time 53239.33ms, mfu 0.73%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 1171: loss 2.7502, time 44423.71ms, mfu 0.74%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 1172: loss 2.9301, time 44293.82ms, mfu 0.74%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 1173: loss 2.7821, time 44121.62ms, mfu 0.74%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 1174: loss 2.6716, time 44024.67ms, mfu 0.74%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 1175: loss 2.8149, time 45408.88ms, mfu 0.74%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 1176: loss 2.7511, time 43974.15ms, mfu 0.75%\n",
      "Will clip_grad_norm\n",
      "Will step\n",
      "Will update\n",
      "iter 1177: loss 3.3231, time 51006.93ms, mfu 0.74%\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "\n",
    "    # determine and set the learning rate for this iteration\n",
    "    lr = get_lr(iter_num) if decay_lr else learning_rate\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "    # evaluate the loss on train/val sets and write checkpoints\n",
    "    if iter_num % eval_interval == 0 and master_process:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter_num}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "        if wandb_log:\n",
    "            wandb.log({\n",
    "                \"iter\": iter_num,\n",
    "                \"train/loss\": losses['train'],\n",
    "                \"val/loss\": losses['val'],\n",
    "                \"lr\": lr,\n",
    "                \"mfu\": running_mfu*100, # convert to percentage\n",
    "            })\n",
    "        if losses['val'] < best_val_loss or always_save_checkpoint:\n",
    "            best_val_loss = losses['val']\n",
    "            if iter_num > 0:\n",
    "                checkpoint = {\n",
    "                    'model': raw_model.state_dict(),\n",
    "                    'optimizer': optimizer.state_dict(),\n",
    "                    'model_args': model_args,\n",
    "                    'iter_num': iter_num,\n",
    "                    'best_val_loss': best_val_loss,\n",
    "                    'config': config,\n",
    "                }\n",
    "                if use_lora:\n",
    "                    checkpoint['lora'] = minlora.get_lora_state_dict(raw_model)\n",
    "                print(f\"saving checkpoint to {out_dir}\")\n",
    "                torch.save(checkpoint, os.path.join(out_dir, 'ckpt.pt'))\n",
    "    if iter_num == 0 and eval_only:\n",
    "        break\n",
    "\n",
    "    # forward backward update, with optional gradient accumulation to simulate larger batch size\n",
    "    # and using the GradScaler if data type is float16\n",
    "    for micro_step in range(gradient_accumulation_steps):\n",
    "        if ddp:\n",
    "            # in DDP training we only need to sync gradients at the last micro step.\n",
    "            # the official way to do this is with model.no_sync() context manager, but\n",
    "            # I really dislike that this bloats the code and forces us to repeat code\n",
    "            # looking at the source of that context manager, it just toggles this variable\n",
    "            model.require_backward_grad_sync = (micro_step == gradient_accumulation_steps - 1)\n",
    "        with ctx:\n",
    "            logits, loss = model(X, Y)\n",
    "        # immediately async prefetch next batch while model is doing the forward pass on the GPU\n",
    "        X, Y = get_batch('train')\n",
    "        # backward pass, with gradient scaling if training in fp16\n",
    "        #print(f'Will scale and backward {micro_step} from {gradient_accumulation_steps}')\n",
    "        scaler.scale(loss).backward()\n",
    "    # clip the gradient\n",
    "    if grad_clip != 0.0:\n",
    "        scaler.unscale_(optimizer)\n",
    "        print('Will clip_grad_norm')\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "    print('Will step')\n",
    "    # step the optimizer and scaler if training in fp16\n",
    "    scaler.step(optimizer)\n",
    "    print('Will update')\n",
    "    scaler.update()\n",
    "    # flush the gradients as soon as we can, no need for this memory anymore\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "    # timing and logging\n",
    "    t1 = time.time()\n",
    "    dt = t1 - t0\n",
    "    t0 = t1\n",
    "    if True or iter_num % log_interval == 0 and master_process:\n",
    "        lossf = loss.item() # loss as float. note: this is a CPU-GPU sync point\n",
    "        if local_iter_num >= 5: # let the training loop settle a bit\n",
    "            mfu = raw_model.estimate_mfu(batch_size * gradient_accumulation_steps, dt)\n",
    "            running_mfu = mfu if running_mfu == -1.0 else 0.9*running_mfu + 0.1*mfu\n",
    "        print(f\"iter {iter_num}: loss {lossf:.4f}, time {dt*1000:.2f}ms, mfu {running_mfu*100:.2f}%\")\n",
    "    iter_num += 1\n",
    "    local_iter_num += 1\n",
    "\n",
    "    # termination conditions\n",
    "    if iter_num > max_iters:\n",
    "        break\n",
    "\n",
    "if ddp:\n",
    "    destroy_process_group()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "nanogpt",
   "language": "python",
   "display_name": "nanogpt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
