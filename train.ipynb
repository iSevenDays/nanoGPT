{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "colab = False\n",
    "try:\n",
    "  import google.colab\n",
    "  colab = True\n",
    "except:\n",
    "  import os\n",
    "  os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'\n",
    "  os.environ[\"TORCH_MPS\"] = \"1\"\n",
    "\n",
    "print(f'colab mode: {colab}')\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "requirements_installed = False\n",
    "if torch.__version__.startswith('2.'):\n",
    "  requirements_installed = True\n",
    "print(f'requirements_installed: {requirements_installed}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if colab:\n",
    "  !git clone https://github.com/iSevenDays/nanoGPT.git\n",
    "  path = !pwd\n",
    "  if 'nanoGPT' not in path:\n",
    "    %cd nanoGPT/\n",
    "  !git pull origin master"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if colab:\n",
    "  !nvidia-smi"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if colab and not requirements_installed:\n",
    "    !pip install accelerate==0.15.0 \\\n",
    "    attrs==22.2.0 \\\n",
    "    bitsandbytes==0.35.4 \\\n",
    "    certifi==2022.12.7 \\\n",
    "    cffi==1.15.1 \\\n",
    "    charset-normalizer==3.1.0 \\\n",
    "    cython==0.29.33 \\\n",
    "    datasets==2.10.1 \\\n",
    "    dill==0.3.6 \\\n",
    "    happytransformer==2.4.1 \\\n",
    "    huggingface-hub==0.11.1 \\\n",
    "    git+https://github.com/cccntu/minLoRA.git@main \\\n",
    "    multiprocess==0.70.14 \\\n",
    "    mwparserfromhell==0.6.4 \\\n",
    "    nltk==3.8.1 \\\n",
    "    numpy \\\n",
    "    protobuf==3.19.5 \\\n",
    "    psutil==5.9.4 \\\n",
    "    python-dateutil==2.8.2 \\\n",
    "    python-json-logger==2.0.7 \\\n",
    "    pytorch-lightning==1.9.4 \\\n",
    "    requests==2.28.2 \\\n",
    "    responses==0.18.0 \\\n",
    "    sentencepiece==0.1.97 \\\n",
    "    six==1.16.0 \\\n",
    "    taming-transformers==0.0.1 \\\n",
    "    tiktoken \\\n",
    "    timm==0.4.12 \\\n",
    "    tokenizers==0.13.2 \\\n",
    "    torch --pre --force-reinstall --extra-index-url https://download.pytorch.org/whl/nightly/cu117 \\\n",
    "    torchmetrics==0.11.3 \\\n",
    "    tqdm==4.65.0 \\\n",
    "    transformers==4.26.1 \\\n",
    "    typing-extensions==4.5.0 \\\n",
    "    urllib3==1.26.14 \\\n",
    "    wandb \\\n",
    "    pathos \\\n",
    "    chardet\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Click on restart runtime!!!"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "from minlora import LoRAParametrization\n",
    "\n",
    "lora_dropout_p = 0.0\n",
    "rank = 256\n",
    "lora_alpha = 256\n",
    "lora_config = {\n",
    "    torch.nn.Embedding: {\n",
    "        \"weight\": partial(LoRAParametrization.from_embedding, rank=rank, lora_alpha=lora_alpha),\n",
    "    },\n",
    "    torch.nn.Linear: {\n",
    "        \"weight\": partial(LoRAParametrization.from_linear, rank=rank, lora_alpha=lora_alpha),\n",
    "    },\n",
    "}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\"\"\"\n",
    "This training script can be run both on a single gpu in debug mode,\n",
    "and also in a larger training run with distributed data parallel (ddp).\n",
    "\n",
    "To run on a single GPU, example:\n",
    "$ python train.py --batch_size=32 --compile=False\n",
    "\n",
    "To run with DDP on 4 gpus on 1 node, example:\n",
    "$ torchrun --standalone --nproc_per_node=4 train.py\n",
    "\n",
    "To run with DDP on 4 gpus across 2 nodes, example:\n",
    "- Run on the first (master) node with example IP 123.456.123.456:\n",
    "$ torchrun --nproc_per_node=8 --nnodes=2 --node_rank=0 --master_addr=123.456.123.456 --master_port=1234 train.py\n",
    "- Run on the worker node:\n",
    "$ torchrun --nproc_per_node=8 --nnodes=2 --node_rank=1 --master_addr=123.456.123.456 --master_port=1234 train.py\n",
    "(If your cluster does not have Infiniband interconnect prepend NCCL_IB_DISABLE=1)\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import math\n",
    "import pickle\n",
    "import inspect\n",
    "from contextlib import nullcontext\n",
    "\n",
    "import torch\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.distributed import init_process_group, destroy_process_group\n",
    "\n",
    "import minlora\n",
    "# -----------------------------------------------------------------------------\n",
    "# default config values designed to train a gpt2 (124M) on OpenWebText\n",
    "# I/O\n",
    "out_dir = 'out'\n",
    "eval_interval = 20\n",
    "log_interval = 1\n",
    "eval_iters = 100 # to calculate loss\n",
    "eval_only = False # if True, script exits right after the first eval\n",
    "always_save_checkpoint = False # if True, always save a checkpoint after each eval\n",
    "init_from = 'scratch' # 'scratch' or 'resume' or 'gpt2*'\n",
    "block_size = 256\n",
    "# wandb logging\n",
    "wandb_log = True # disabled by default\n",
    "use_lora = False\n",
    "# model\n",
    "n_layer = 16\n",
    "n_head = 20\n",
    "n_embd = 320\n",
    "dropout = 0.0 # for pretraining 0 is good, for finetuning try 0.1+\n",
    "bias = False # do we use bias inside LayerNorm and Linear layers?\n",
    "\n",
    "\n",
    "#### RWKV specific\n",
    "# model\n",
    "ctx_len = 256\n",
    "rwkv_emb_scale = 0.4\n",
    "rwkv_tiny_attn = 2\n",
    "rwkv_tiny_head = 2\n",
    "n_attn = n_embd\n",
    "n_ffn = n_embd\n",
    "### RWKV END\n",
    "\n",
    "from enum import Enum\n",
    "\n",
    "class ModelType(Enum):\n",
    "    ChatGPT = 1\n",
    "    Lama = 2\n",
    "    Hyena = 3\n",
    "    RWKV = 4\n",
    "\n",
    "model_type = ModelType.RWKV\n",
    "\n",
    "wandb_project = f'united_medium'\n",
    "\n",
    "if model_type == ModelType.Hyena:\n",
    "    if use_lora:\n",
    "        checkpoint_name = f'united_medium_{block_size}-gpthyena-lora-{rank}_{lora_alpha}_nl{n_layer}_ne{n_embd}.pt'\n",
    "        wandb_run_name = f'gpt2-hyena-lora-{rank}_{lora_alpha}_nl{n_layer}_ne{n_embd}'\n",
    "    else:\n",
    "        checkpoint_name = f'united_medium_{block_size}-gpthyena-adam_nl{n_layer}_ne{n_embd}.pt'\n",
    "        wandb_run_name = f'gpt2-hyena-adam_nl{n_layer}_ne{n_embd}'\n",
    "elif model_type == ModelType.RWKV:\n",
    "    if use_lora:\n",
    "        checkpoint_name = f'united_medium_{block_size}-gptrwkv-lora-{rank}_{lora_alpha}_nl{n_layer}_ne{n_embd}.pt'\n",
    "        wandb_run_name = f'gpt2-rwkv-lora-{rank}_{lora_alpha}_nl{n_layer}_ne{n_embd}_attn{rwkv_tiny_attn}'\n",
    "    else:\n",
    "        checkpoint_name = f'united_medium_{block_size}-gptrwkv-adam_nl{n_layer}_ne{n_embd}.pt'\n",
    "        wandb_run_name = f'gpt2-rwkv-adam_nl{n_layer}_ne{n_embd}'\n",
    "elif model_type == ModelType.Lama:\n",
    "    wandb_project = f'united_medium'\n",
    "    if use_lora:\n",
    "        checkpoint_name = f'united_medium_{block_size}-gptlama-lora-{rank}_{lora_alpha}_nl{n_layer}_ne{n_embd}.pt'\n",
    "        wandb_run_name = f'gpt2-lama-lora-{rank}_{lora_alpha}_nl{n_layer}_ne{n_embd}'\n",
    "    else:\n",
    "        checkpoint_name = f'united_medium_{block_size}-gptlama-adam_nl{n_layer}_ne{n_embd}.pt'\n",
    "        wandb_run_name = f'gpt2-lama-adam_nl{n_layer}_ne{n_embd}'\n",
    "elif model_type == ModelType.ChatGPT:\n",
    "    checkpoint_name = f'united_medium_{block_size}-gpt.pt'\n",
    "    wandb_run_name = f'gpt2_nl{n_layer}_ne{n_embd}'\n",
    "\n",
    "if model_type == ModelType.RWKV:\n",
    "    from rwkv import GPT, GPTConfig\n",
    "else:\n",
    "    from model import GPTConfig, GPT\n",
    "# data\n",
    "dataset = 'united'\n",
    "gradient_accumulation_steps = 5 # used to simulate larger batch sizes\n",
    "batch_size = 14#24 # ~8 for A100; if gradient_accumulation_steps > 1, this is the micro-batch size\n",
    "\n",
    "# adamw optimizer\n",
    "learning_rate = 1e-3 #1e-3 for lora #6e-4 # max learning rate\n",
    "max_iters = 10000 # total number of training iterations\n",
    "weight_decay = 1e-1\n",
    "beta1 = 0.9\n",
    "beta2 = 0.95\n",
    "grad_clip = 1.0 # clip gradients at this value, or disable if == 0.0\n",
    "# learning rate decay settings\n",
    "decay_lr = True # whether to decay the learning rate\n",
    "warmup_iters = 20 # how many steps to warm up for\n",
    "lr_decay_iters = 10000 # should be ~= max_iters per Chinchilla\n",
    "min_lr = 9e-5 # minimum learning rate, should be ~= learning_rate/10 per Chinchilla\n",
    "# DDP settings\n",
    "backend = 'nccl' # 'nccl', 'gloo', etc.\n",
    "# system\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = 'mps'\n",
    "else:\n",
    "    device = 'cpu'\n",
    " # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1' etc., or try 'mps' on macbooks\n",
    "dtype = 'float16' if colab else 'float16' # 'float32', 'bfloat16', or 'float16', the latter will auto implement a GradScaler\n",
    "compile = False # use PyTorch 2.0 to compile the model to be faster\n",
    "# -----------------------------------------------------------------------------\n",
    "config_keys = [k for k,v in globals().items() if not k.startswith('_') and isinstance(v, (int, float, bool, str))]\n",
    "#exec(open('configurator.py').read()) # overrides from command line or config file\n",
    "config = {k: globals()[k] for k in config_keys} # will be useful for logging\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# various inits, derived attributes, I/O setup\n",
    "ddp = int(os.environ.get('RANK', -1)) != -1 # is this a ddp run?\n",
    "if ddp:\n",
    "    print('Will use ddp')\n",
    "    init_process_group(backend=backend)\n",
    "    ddp_rank = int(os.environ['RANK'])\n",
    "    ddp_local_rank = int(os.environ['LOCAL_RANK'])\n",
    "    device = f'cuda:{ddp_local_rank}'\n",
    "    torch.cuda.set_device(device)\n",
    "    master_process = ddp_rank == 0 # this process will do logging, checkpointing etc.\n",
    "    seed_offset = ddp_rank # each process gets a different seed\n",
    "else:\n",
    "    print('Will not use ddp')\n",
    "    # if not ddp, we are running on a single gpu, and one process\n",
    "    master_process = True\n",
    "    seed_offset = 0\n",
    "    gradient_accumulation_steps *= 8 # simulate 8 gpus\n",
    "\n",
    "if master_process:\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "torch.manual_seed(1337 + seed_offset)\n",
    "if colab:\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True # allow tf32 on matmul\n",
    "    torch.backends.cudnn.allow_tf32 = True # allow tf32 on cudnn\n",
    "device_type = 'cuda' if 'cuda' in device else 'cpu' # for later use in torch.autocast\n",
    "# note: float16 data type will automatically use a GradScaler\n",
    "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
    "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "assert n_embd % n_head == 0"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data_dir = os.path.join('data', dataset)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if colab:\n",
    "  from google.colab import drive\n",
    "  drive.mount('/content/drive')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if colab:\n",
    "  !ls /content/drive/MyDrive/gpt-checkpoints\n",
    "  out_dir = '/content/drive/MyDrive/gpt-checkpoints'\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# init these up here, can override if init_from='resume' (i.e. from a checkpoint)\n",
    "iter_num = 0\n",
    "best_val_loss = 1e9\n",
    "tokens_seen = 0\n",
    "\n",
    "# attempt to derive vocab_size from the dataset\n",
    "meta_path = os.path.join(data_dir, 'meta.pkl')\n",
    "meta_vocab_size = None\n",
    "if os.path.exists(meta_path):\n",
    "    with open(meta_path, 'rb') as f:\n",
    "        meta = pickle.load(f)\n",
    "    meta_vocab_size = meta['vocab_size']\n",
    "    print(f\"found vocab_size = {meta_vocab_size} (inside {meta_path})\")\n",
    "\n",
    "# model init\n",
    "if model_type == ModelType.RWKV:\n",
    "    vocab_size = meta_vocab_size if meta_vocab_size is not None else 50304\n",
    "    model_args = dict(vocab_size=vocab_size, ctx_len=ctx_len, model_type=\"RWKV\",\n",
    "                  rwkv_emb_scale=rwkv_emb_scale, rwkv_tiny_attn=rwkv_tiny_attn, rwkv_tiny_head=rwkv_tiny_head,\n",
    "                  n_layer=n_layer, n_head=n_head, n_embd=n_embd, n_attn=n_attn,\n",
    "                  n_ffn=n_ffn)\n",
    "else:\n",
    "    model_args = dict(n_layer=n_layer, n_head=n_head, n_embd=n_embd, block_size=block_size,\n",
    "                  bias=bias, vocab_size=None, dropout=dropout, hyena=model_type==ModelType.Hyena,\n",
    "                  rotary=model_type==ModelType.Lama) # start with model_args from command line\n",
    "if init_from == 'scratch':\n",
    "    # init a new model from scratch\n",
    "    print(\"Initializing a new model from scratch\")\n",
    "    # determine the vocab size we'll use for from-scratch training\n",
    "    if meta_vocab_size is None:\n",
    "        print(\"defaulting to vocab_size of GPT-2 to 50304 (50257 rounded up for efficiency)\")\n",
    "    model_args['vocab_size'] = meta_vocab_size if meta_vocab_size is not None else 50304\n",
    "    gptconf = GPTConfig(**model_args)\n",
    "    model = GPT(gptconf)\n",
    "elif init_from == 'resume':\n",
    "    print(f\"Resuming training from {out_dir}\")\n",
    "    # resume training from a checkpoint.\n",
    "    ckpt_path = os.path.join(out_dir, checkpoint_name)\n",
    "    checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "    checkpoint_model_args = checkpoint['model_args']\n",
    "    # force these config attributes to be equal otherwise we can't even resume training\n",
    "    # the rest of the attributes (e.g. dropout) can stay as desired from command line\n",
    "    for k in ['n_layer', 'n_head', 'n_embd', 'block_size', 'bias', 'vocab_size']:\n",
    "        try:\n",
    "            val = checkpoint_model_args[k]\n",
    "        except KeyError:\n",
    "            continue\n",
    "        model_args[k] = val\n",
    "        if k == 'n_layer' and n_layer != val:\n",
    "          print(f'Override {k} from {n_layer} to {val}')\n",
    "          n_layer = val\n",
    "        if k == 'n_head' and n_head != val:\n",
    "          print(f'Override {k} from {n_head} to {val}')\n",
    "          n_head = val\n",
    "        if k == 'n_embd' and n_embd != val:\n",
    "          print(f'Override {k} from {n_embd} to {val}')\n",
    "          n_embd = val\n",
    "    # create the model\n",
    "    gptconf = GPTConfig(**model_args)\n",
    "    model = GPT(gptconf)\n",
    "    if use_lora:\n",
    "        minlora.add_lora(model, lora_config)\n",
    "\n",
    "    state_dict = checkpoint['model']\n",
    "    # fix the keys of the state dictionary :(\n",
    "    # honestly no idea how checkpoints sometimes get this prefix, have to debug more\n",
    "    unwanted_prefix = '_orig_mod.'\n",
    "    for k,v in list(state_dict.items()):\n",
    "        if k.startswith(unwanted_prefix):\n",
    "            state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
    "\n",
    "    model.load_state_dict(state_dict)\n",
    "    if use_lora:\n",
    "        # the full state dict includes the LoRA state dict\n",
    "        # so actually we don't need to load it separately again\n",
    "        model.load_state_dict(checkpoint['lora'], strict=False)\n",
    "        print('Loaded LoRA state dict')\n",
    "        # sanity check\n",
    "        #model.apply(minlora.apply_to_lora(lambda m: print((m.lora_A.sum(), m.lora_B.sum()))))\n",
    "        # merge for zero-overhead inference\n",
    "        minlora.merge_lora(model)\n",
    "\n",
    "    iter_num = checkpoint['iter_num']\n",
    "    best_val_loss = checkpoint['best_val_loss']\n",
    "    tokens_seen = checkpoint.get('tokens_seen', 0)\n",
    "\n",
    "    if tokens_seen == 0:\n",
    "        tokens_seen = batch_size * gradient_accumulation_steps * n_embd\n",
    "elif init_from.startswith('gpt2'):\n",
    "    print(f\"Initializing from OpenAI GPT-2 weights: {init_from}\")\n",
    "    # initialize from OpenAI GPT-2 weights\n",
    "    override_args = dict(dropout=dropout)\n",
    "    model = GPT.from_pretrained(init_from, override_args)\n",
    "    # read off the created config params, so we can store them into checkpoint correctly\n",
    "    for k in ['n_layer', 'n_head', 'n_embd', 'block_size', 'bias', 'vocab_size']:\n",
    "        model_args[k] = getattr(model.config, k)\n",
    "# crop down the model block size if desired, using model surgery\n",
    "if model_type != ModelType.RWKV:\n",
    "    if block_size < model.config.block_size:\n",
    "        model.crop_block_size(block_size)\n",
    "        model_args['block_size'] = block_size # so that the checkpoint will have the right value\n",
    "if use_lora:\n",
    "    print(\"Using lora\")\n",
    "    minlora.add_lora(model, lora_config=lora_config)\n",
    "    if model_type == ModelType.RWKV:\n",
    "        minlora.tie_weights(linear=model.head, embedding=model.tok_emb)\n",
    "    else:\n",
    "        minlora.tie_weights(linear=model.lm_head, embedding=model.transformer.wte)\n",
    "model.to(device)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#!python -m spacy download en_core_web_sm"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import importlib\n",
    "import CachedDataLoader\n",
    "importlib.reload(CachedDataLoader)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "MAX_LENGTH = block_size"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from CachedDataLoader import CachedDataLoader\n",
    "import time\n",
    "from transformers import GPT2Tokenizer\n",
    "import torch\n",
    "\n",
    "#\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "BUFFER_SIZE = 10000\n",
    "#\n",
    "MAX_LENGTH = block_size\n",
    "parallel = True\n",
    "use_pile = False\n",
    "# Define a function to download and preprocess the data in a background thread\n",
    "\n",
    "dataset_weights = {\n",
    "    'pile': 0.6,\n",
    "    'wiki': 0.1,\n",
    "    'bookcorpus': 0.3\n",
    "} if use_pile else {'wiki':0.1, 'bookcorpus': 0.9}\n",
    "\n",
    "datasets=['wiki', 'pile'] if use_pile else ['wiki', 'bookcorpus']\n",
    "\n",
    "data_loader = CachedDataLoader(datasets, dataset_weights)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "#cache_dir = '/Volumes/WD_RED_RW/machine_learning/cache'\n",
    "#cache_dir = '/Volumes/homes/seven/Btrfs_main_backup/Macbook14M1Leipzig.local/Volumes/WD_RED_RW/machine_learning/cache'\n",
    "dataset = load_dataset(\"wikitext\", 'wikitext-103-v1', split=\"train\")\n",
    "#dataset = load_dataset(\"EleutherAI/the_pile_deduplicated\", split=\"train\", cache_dir=cache_dir)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "shard = dataset.shard(num_shards=128, index=0)\n",
    "shard"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "split = shard.train_test_split(test_size=0.1, seed=42)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_dataset = split['train']\n",
    "test_dataset = split['test']\n",
    "split['train'][0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# cache_dir = '/Users/seven/cache'\n",
    "# from datasets import load_from_disk\n",
    "# train_cache_file = f\"{cache_dir}/train_split.arrow\"\n",
    "# test_cache_file = f\"{cache_dir}/test_split.arrow\"\n",
    "# if os.path.exists(train_cache_file):\n",
    "#     train_dataset = load_from_disk(train_cache_file)\n",
    "# else:\n",
    "#     split['train'].save_to_disk(train_cache_file)\n",
    "#\n",
    "# if os.path.exists(test_cache_file):\n",
    "#     test_dataset = load_from_disk(test_cache_file)\n",
    "# else:\n",
    "#     split['test'].save_to_disk(test_cache_file)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import random\n",
    "def randomly_select_element(dataset):\n",
    "    num_rows = len(dataset)\n",
    "    random_index = random.randint(0, num_rows - 1)\n",
    "    random_element = dataset[random_index]\n",
    "    return random_element\n",
    "\n",
    "randomly_select_element(train_dataset)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from IrrelevantChecker import IrrelevantChecker\n",
    "import importlib\n",
    "import text_cleaner\n",
    "importlib.reload(text_cleaner)\n",
    "from text_cleaner import TextCleaner, TextCleaningException\n",
    "\n",
    "textcleaner = TextCleaner()\n",
    "\n",
    "from is_irrelevant import is_irrelevant\n",
    "relevance_checker = IrrelevantChecker(is_irrelevant)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def randomly_select_valid_clean_element(dataset, debug=False):\n",
    "    result_text = None\n",
    "    original_text = None\n",
    "    while result_text is None:\n",
    "        element = randomly_select_element(dataset)\n",
    "        try:\n",
    "            original_text = element['text']\n",
    "            result_text = textcleaner.clean(original_text)\n",
    "            if not relevance_checker.is_relevant(result_text):\n",
    "                if debug:\n",
    "                    print(f'Skipping not relevant: {result_text}')\n",
    "                result_text = None\n",
    "        except TextCleaningException as e:\n",
    "            if e.error_reason is text_cleaner.CleaningError.INSUFFICIENT_SENTENCES:\n",
    "                if debug:\n",
    "                    print(f\"Text cleaning failed: {e}, cleaned_text: {e.cleaned_text}, original_text: {element['text']}\")\n",
    "            else:\n",
    "                if debug:\n",
    "                    print(f\"Text cleaning failed: {e}\")\n",
    "    if debug:\n",
    "        print(f\"Text cleaning finished, original text:\\n{original_text}\")\n",
    "    return result_text\n",
    "\n",
    "# Example usage\n",
    "random_element = randomly_select_valid_clean_element(train_dataset, debug=True)\n",
    "print('Found text')\n",
    "print(random_element)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_train_item_slow():\n",
    "    text = randomly_select_valid_clean_element(train_dataset)\n",
    "    ids = tokenizer.encode(text, truncation=True)\n",
    "    ids.append(tokenizer.eos_token_id)\n",
    "    return ids\n",
    "\n",
    "def get_validation_item_slow():\n",
    "    text = randomly_select_valid_clean_element(test_dataset)\n",
    "    ids = tokenizer.encode(text, truncation=True)\n",
    "    ids.append(tokenizer.eos_token_id)\n",
    "    return ids"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from queue import Queue\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "queue_train = Queue(maxsize=1000)  # Adjust the maxsize to control the maximum number of prefetched items\n",
    "queue_validation = Queue(maxsize=1000)\n",
    "\n",
    "def prefetch_train_items(queue_train):\n",
    "    while True:\n",
    "        item = get_train_item_slow()\n",
    "        queue_train.put(item)\n",
    "\n",
    "def prefetch_validation_items(queue_validation):\n",
    "    while True:\n",
    "        item = get_validation_item_slow()\n",
    "        queue_validation.put(item)\n",
    "\n",
    "# Create the ThreadPoolExecutor\n",
    "executor = ThreadPoolExecutor(max_workers=2)\n",
    "\n",
    "# Start the prefetching threads\n",
    "executor.submit(prefetch_train_items, queue_train)\n",
    "executor.submit(prefetch_validation_items, queue_validation)\n",
    "\n",
    "print('started')\n",
    "\n",
    "# You can execute other code here without waiting for the threads\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "use_data_loader = True\n",
    "def get_train_item():\n",
    "    if use_data_loader:\n",
    "        return data_loader.get_train_item()\n",
    "    try:\n",
    "        return queue_train.get(timeout=2)\n",
    "    except:\n",
    "        print('Getting slow item train')\n",
    "        return get_train_item_slow()\n",
    "\n",
    "    #return get_train_item_slow()\n",
    "\n",
    "def get_validation_item():\n",
    "    if use_data_loader:\n",
    "        return data_loader.get_validation_item()\n",
    "    try:\n",
    "        return queue_validation.get(timeout=2)\n",
    "    except:\n",
    "        print('Getting slow item validation')\n",
    "        return get_validation_item_slow()\n",
    "    #return get_validation_item_slow()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_train_queue_size():\n",
    "    if use_data_loader:\n",
    "        return data_loader.get_train_queue_size()\n",
    "    return queue_train.qsize()\n",
    "\n",
    "def get_validation_queue_size():\n",
    "    if use_data_loader:\n",
    "        return data_loader.get_validation_queue_size()\n",
    "    return queue_validation.qsize()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_batch(split, print_prompt_target_texts: bool = False, print_queue_change = True):\n",
    "    buffer_ids = []\n",
    "    result_ids = []\n",
    "    # if split == 'train':\n",
    "    #     queue_size_old = get_train_queue_size()\n",
    "    # else:\n",
    "    #     queue_size_old = get_validation_queue_size()\n",
    "    i = 0\n",
    "    global data_loader\n",
    "    while True:\n",
    "        if len(result_ids) == batch_size:\n",
    "            break\n",
    "        if split == 'train':\n",
    "            ids = get_train_item()\n",
    "        else:\n",
    "            ids = get_validation_item()\n",
    "\n",
    "        buffer_ids.extend(ids)\n",
    "\n",
    "        while len(buffer_ids) > MAX_LENGTH + 1:\n",
    "            if len(result_ids) == batch_size:\n",
    "                break\n",
    "            # Truncate text if it's longer than MAX_LENGTH\n",
    "            id = buffer_ids[:MAX_LENGTH + 1]\n",
    "            result_ids.append(id)\n",
    "            buffer_ids = buffer_ids[MAX_LENGTH:]\n",
    "\n",
    "        i = i + 1\n",
    "\n",
    "    # if split == 'train':\n",
    "    #     queue_size = get_train_queue_size()\n",
    "    # else:\n",
    "    #     queue_size = get_validation_queue_size()\n",
    "    #diff = queue_size_old - queue_size\n",
    "    #print(f'Split {split} took: {diff} elements for {len(result_ids)} elements of {MAX_LENGTH} size (of {queue_size} left)')\n",
    "    x = torch.stack([torch.tensor(ids[:-1]) for ids in result_ids])\n",
    "    y = torch.stack([torch.tensor(ids[1:]) for ids in result_ids])\n",
    "\n",
    "    if print_prompt_target_texts:\n",
    "        # Decode x back to text\n",
    "        prompt_text = [tokenizer.batch_decode(ids) for ids in x]\n",
    "        target_text = [tokenizer.batch_decode(ids) for ids in y]\n",
    "\n",
    "        # Print prompts\n",
    "        for i in range(len(prompt_text)):\n",
    "            print(f\"Prompt {i + 1}: {''.join(prompt_text[i][:60])}\")\n",
    "            print(f\"Target {i + 1}: {''.join(target_text[i][:60])}\")\n",
    "\n",
    "    if device == 'cuda':\n",
    "        # Pin arrays x, y, which allows us to move them to GPU asynchronously (non_blocking=True)\n",
    "        x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True)\n",
    "    else:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "#get_batch('train')\n",
    "#get_batch('train')\n",
    "x, y = get_batch('train', print_prompt_target_texts = True)\n",
    "# x, y = get_batch('train', print_prompt_target_texts = True)\n",
    "# x, y = get_batch('train', print_prompt_target_texts = True)\n",
    "# x, y = get_batch('train', print_prompt_target_texts = True)\n",
    "# x, y = get_batch('train', print_prompt_target_texts = True)\n",
    "# x, y = get_batch('train', print_prompt_target_texts = True)\n",
    "print(f'x shape: {x.shape}')  # should print \"x shape: torch.Size([12, 1024])\"\n",
    "print(f'y shape: {y.shape}')  # should print \"y shape: torch.Size([12, 1024])\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import google.protobuf\n",
    "print(google.protobuf.__version__)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
    "\n",
    "# optimizer\n",
    "def configure_optimizers_lora(self, weight_decay, learning_rate, betas, device_type):\n",
    "    # we apply weight decay to all lora params\n",
    "    optim_groups = [\n",
    "        # note: .get_lora_params() returns a generator\n",
    "        # we need to wrap it in a list so we can consume it twice\n",
    "        {\"params\": list(minlora.get_lora_params(self)) , \"weight_decay\": weight_decay},\n",
    "        # you can also add biases for fine-tuning,\n",
    "        # but I want to make sure lora alone works\n",
    "        # {\"params\": minlora.get_bias_params(self), \"weight_decay\": 0.0}, # bias params don't get weight decay\n",
    "    ]\n",
    "\n",
    "    def parameter_count(optim_groups):\n",
    "        n = sum(p.numel() for group in optim_groups for p in group[\"params\"])\n",
    "        if n < 1e6:\n",
    "            return f\"{n/1e3:.1f}k\"\n",
    "        else:\n",
    "            return f\"{n/1e6:.1f}M\"\n",
    "\n",
    "    print(f\"optimizing {parameter_count(optim_groups)} parameters\")\n",
    "\n",
    "    # new PyTorch nightly has a new 'fused' option for AdamW that is much faster\n",
    "    use_fused = (device_type == \"cuda\") and (\"fused\" in inspect.signature(torch.optim.AdamW).parameters)\n",
    "    print(f\"using fused AdamW: {use_fused}\")\n",
    "    extra_args = dict(fused=True) if use_fused else dict()\n",
    "    optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=betas, **extra_args)\n",
    "\n",
    "    return optimizer\n",
    "if use_lora:\n",
    "    optimizer = configure_optimizers_lora(model, weight_decay, learning_rate, (beta1, beta2), device_type)\n",
    "else:\n",
    "    optimizer = model.configure_optimizers(weight_decay, learning_rate, (beta1, beta2), device_type)\n",
    "if init_from == 'resume':\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "\n",
    "    # disable the fused option\n",
    "    for group in optimizer.param_groups:\n",
    "        if group['fused'] == True and device != 'cuda':\n",
    "            print('Disabling previously enabled `fused` option')\n",
    "        elif group['fused'] == False and device == 'cuda':\n",
    "            print('Enabling previously disabled `fused` option')\n",
    "        group['fused'] = device == 'cuda'\n",
    "\n",
    "if device == 'mps':\n",
    "    torch.backends.mps.enabled = True\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(device)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# compile the model\n",
    "if compile:\n",
    "    print(\"compiling the model... (takes a ~minute)\")\n",
    "    unoptimized_model = model\n",
    "    model = torch.compile(model) # requires PyTorch 2.0\n",
    "\n",
    "# wrap model into DDP container\n",
    "if ddp:\n",
    "    model = DDP(model, device_ids=[ddp_local_rank])\n",
    "\n",
    "# helps estimate an arbitrarily accurate loss over either split using many batches\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            with ctx:\n",
    "                logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "# learning rate decay scheduler (cosine with warmup)\n",
    "def get_lr(it):\n",
    "    # 1) linear warmup for warmup_iters steps\n",
    "    if it < warmup_iters:\n",
    "        return learning_rate * it / warmup_iters\n",
    "    # 2) if it > lr_decay_iters, return min learning rate\n",
    "    if it > lr_decay_iters:\n",
    "        return min_lr\n",
    "    # 3) in between, use cosine decay down to min learning rate\n",
    "    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n",
    "    assert 0 <= decay_ratio <= 1\n",
    "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff ranges 0..1\n",
    "    return min_lr + coeff * (learning_rate - min_lr)\n",
    "\n",
    "# logging\n",
    "if wandb_log and master_process:\n",
    "    try:\n",
    "        if wandb_init_completed:\n",
    "            print('wand already initialized')\n",
    "    except:\n",
    "        wandb_init_completed = True\n",
    "        import wandb\n",
    "\n",
    "        if colab:\n",
    "            \n",
    "            wandb.login(key=wb_key)\n",
    "\n",
    "        wandb.init(project=wandb_project, name=wandb_run_name, config=config)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# training loop\n",
    "X, Y = get_batch('train') # fetch the very first batch\n",
    "t0 = time.time()\n",
    "local_iter_num = 0 # number of iterations in the lifetime of this process\n",
    "raw_model = model.module if ddp else model # unwrap DDP container if needed\n",
    "running_mfu = -1.0"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "raw_model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "while True:\n",
    "\n",
    "    # determine and set the learning rate for this iteration\n",
    "    lr = get_lr(iter_num) if decay_lr else learning_rate\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "    # evaluate the loss on train/val sets and write checkpoints\n",
    "    if iter_num % eval_interval == 0 and master_process:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter_num}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "        if wandb_log:\n",
    "            wandb.log({\n",
    "                \"iter\": iter_num,\n",
    "                \"train/loss\": losses['train'],\n",
    "                \"val/loss\": losses['val'],\n",
    "                \"lr\": lr,\n",
    "                \"mfu\": running_mfu*100, # convert to percentage\n",
    "                'tokens_seen': tokens_seen,\n",
    "            }, step=int(iter_num/eval_interval))\n",
    "        if losses['val'] < best_val_loss or always_save_checkpoint:\n",
    "            best_val_loss = losses['val']\n",
    "            if iter_num > 0:\n",
    "                checkpoint = {\n",
    "                    'model': raw_model.state_dict(),\n",
    "                    'optimizer': optimizer.state_dict(),\n",
    "                    'model_args': model_args,\n",
    "                    'iter_num': iter_num,\n",
    "                    'best_val_loss': best_val_loss,\n",
    "                    'tokens_seen': tokens_seen,\n",
    "                    'config': config,\n",
    "                }\n",
    "                if use_lora:\n",
    "                    checkpoint['lora'] = minlora.get_lora_state_dict(raw_model)\n",
    "                print(f\"saving checkpoint to {out_dir}\")\n",
    "                torch.save(checkpoint, os.path.join(out_dir, checkpoint_name))\n",
    "    if iter_num == 0 and eval_only:\n",
    "        break\n",
    "\n",
    "    # forward backward update, with optional gradient accumulation to simulate larger batch size\n",
    "    # and using the GradScaler if data type is float16\n",
    "    for micro_step in range(gradient_accumulation_steps):\n",
    "        if ddp:\n",
    "            # in DDP training we only need to sync gradients at the last micro step.\n",
    "            # the official way to do this is with model.no_sync() context manager, but\n",
    "            # I really dislike that this bloats the code and forces us to repeat code\n",
    "            # looking at the source of that context manager, it just toggles this variable\n",
    "            model.require_backward_grad_sync = (micro_step == gradient_accumulation_steps - 1)\n",
    "        with ctx:\n",
    "            logits, loss = model(X, Y)\n",
    "        # immediately async prefetch next batch while model is doing the forward pass on the GPU\n",
    "        #print(f'Will get x, y')\n",
    "        X, Y = get_batch('train')\n",
    "        # backward pass, with gradient scaling if training in fp16\n",
    "        #print(f'Will scale and backward {micro_step} from {gradient_accumulation_steps}')\n",
    "        scaler.scale(loss).backward()\n",
    "    # clip the gradient\n",
    "    if grad_clip != 0.0:\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "    # step the optimizer and scaler if training in fp16\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "    # flush the gradients as soon as we can, no need for this memory anymore\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "    # timing and logging\n",
    "    t1 = time.time()\n",
    "    dt = t1 - t0\n",
    "    t0 = t1\n",
    "    if iter_num % log_interval == 0 and master_process:\n",
    "        lossf = loss.item() # loss as float. note: this is a CPU-GPU sync point\n",
    "        if local_iter_num >= 5: # let the training loop settle a bit\n",
    "            mfu = raw_model.estimate_mfu(batch_size * gradient_accumulation_steps, dt)\n",
    "            running_mfu = mfu if running_mfu == -1.0 else 0.9*running_mfu + 0.1*mfu\n",
    "        print(f\"iter {iter_num}: loss {lossf:.4f}, time {dt*1000:.2f}ms, mfu {running_mfu*100:.2f}%\")\n",
    "    iter_num += 1\n",
    "    local_iter_num += 1\n",
    "    tokens_seen += batch_size * gradient_accumulation_steps * n_embd\n",
    "\n",
    "    # termination conditions\n",
    "    if iter_num > max_iters:\n",
    "        break\n",
    "\n",
    "if ddp:\n",
    "    destroy_process_group()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "nanogpt",
   "language": "python",
   "display_name": "nanogpt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
